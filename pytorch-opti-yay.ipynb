{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "!pip install -q torchviz\n",
    "!pip install -q colored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "import colored\n",
    "from colored import fg, bg, attr\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import randint\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torchviz import make_dot\n",
    "torch.backends.cudnn.benchmark = True\n",
    "from torchvision import transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torchvision.models import resnet18, densenet121, mobilenet_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 2\n",
    "SPLIT = 0.8\n",
    "LR = (1e-4, 1e-3)\n",
    "MODEL_SAVE_PATH = \"resnet_model\"\n",
    "\n",
    "W = 64\n",
    "H = 64\n",
    "BATCH_SIZE = 32\n",
    "VAL_BATCH_SIZE = 32\n",
    "DATA_PATH = '../input/trends-assessment-prediction/'\n",
    "deep = 63"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Normalize(mean=[0.485 0.456 0.406 0.485 0.456 0.406 0.485 0.456 0.406 0.485 0.456 0.406\n",
       " 0.485 0.456 0.406 0.485 0.456 0.406 0.485 0.456 0.406 0.485 0.456 0.406\n",
       " 0.485 0.456 0.406 0.485 0.456 0.406 0.485 0.456 0.406 0.485 0.456 0.406\n",
       " 0.485 0.456 0.406 0.485 0.456 0.406 0.485 0.456 0.406 0.485 0.456 0.406\n",
       " 0.485 0.456 0.406 0.485 0.456 0.406 0.485 0.456 0.406 0.485 0.456 0.406\n",
       " 0.485 0.456 0.406], std=[0.229 0.224 0.225 0.229 0.224 0.225 0.229 0.224 0.225 0.229 0.224 0.225\n",
       " 0.229 0.224 0.225 0.229 0.224 0.225 0.229 0.224 0.225 0.229 0.224 0.225\n",
       " 0.229 0.224 0.225 0.229 0.224 0.225 0.229 0.224 0.225 0.229 0.224 0.225\n",
       " 0.229 0.224 0.225 0.229 0.224 0.225 0.229 0.224 0.225 0.229 0.224 0.225\n",
       " 0.229 0.224 0.225 0.229 0.224 0.225 0.229 0.224 0.225 0.229 0.224 0.225\n",
       " 0.229 0.224 0.225])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize = transforms.Normalize(mean=np.tile([0.485, 0.456, 0.406], 21), std=np.tile([0.229, 0.224, 0.225], 21))\n",
    "normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_MAP_PATH = DATA_PATH + 'fMRI_test/'\n",
    "TRAIN_MAP_PATH = DATA_PATH + 'fMRI_train/'\n",
    "\n",
    "FEAT_PATH = DATA_PATH + 'fnc.csv'\n",
    "TARG_PATH = DATA_PATH + 'train_scores.csv'\n",
    "SAMPLE_SUB_PATH = DATA_PATH + 'sample_submission.csv'\n",
    "\n",
    "TEST_IDS = [map_id[:-4] for map_id in sorted(os.listdir(TEST_MAP_PATH))]\n",
    "TRAIN_IDS = [map_id[:-4] for map_id in sorted(os.listdir(TRAIN_MAP_PATH))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = pd.read_csv(TARG_PATH)\n",
    "targets = targets.fillna(targets.mean())\n",
    "sample_submission = pd.read_csv(SAMPLE_SUB_PATH)\n",
    "\n",
    "features = pd.read_csv(FEAT_PATH)\n",
    "test_df = features.query('Id in {}'.format(TEST_IDS)).reset_index(drop=True)\n",
    "train_df = features.query('Id in {}'.format(TRAIN_IDS)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnc_df = pd.read_csv(\"/kaggle/input/trends-assessment-prediction/fnc.csv\")\n",
    "loading_df = pd.read_csv(\"/kaggle/input/trends-assessment-prediction/loading.csv\")\n",
    "\n",
    "\n",
    "fnc_features, loading_features = list(fnc_df.columns[1:]), list(loading_df.columns[1:])\n",
    "df_full = fnc_df.merge(loading_df, on=\"Id\")\n",
    "\n",
    "\n",
    "labels_df = pd.read_csv(\"/kaggle/input/trends-assessment-prediction/train_scores.csv\")\n",
    "labels_df[\"is_train\"] = True\n",
    "\n",
    "df = df_full.merge(labels_df, on=\"Id\", how=\"left\")\n",
    "df_full = df_full.set_index('Id')\n",
    "\n",
    "test_df_x = df[df[\"is_train\"] != True].copy()\n",
    "df = df[df[\"is_train\"] == True].copy()\n",
    "\n",
    "df.shape, test_df_x.shape\n",
    "# Giving less importance to FNC features since they are easier to overfit due to high dimensionality.\n",
    "FNC_SCALE = 1/500\n",
    "\n",
    "df[fnc_features] *= FNC_SCALE\n",
    "test_df_x[fnc_features] *= FNC_SCALE\n",
    "df_full[fnc_features]  *= FNC_SCALE\n",
    "\n",
    "features = loading_features + fnc_features\n",
    "\n",
    " #######\n",
    "    # do not forget to do something about the mean\n",
    "# selecting best features\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "targets_list = [\"age\", \"domain1_var1\", \"domain1_var2\", \"domain2_var1\", \"domain2_var2\"]\n",
    "\n",
    "best_columns = {}\n",
    "for target in targets_list:\n",
    "    X, y = df[features], df[target]\n",
    "    y.fillna(y.mean(), inplace=True)\n",
    "    selector = SelectKBest(f_regression, k=128)\n",
    "    X = selector.fit_transform(X, y)    \n",
    "    best_columns[target] = selector.get_support(indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_values = []\n",
    "for target in targets_list:\n",
    "    for val in best_columns[target]:\n",
    "        column_values.append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "c = Counter(column_values)\n",
    "final_columns = list(df[features].iloc[:, [i for i, _ in c.most_common(128)]].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IC_06</th>\n",
       "      <th>IC_15</th>\n",
       "      <th>IC_21</th>\n",
       "      <th>IC_28</th>\n",
       "      <th>SMN(2)_vs_SCN(53)</th>\n",
       "      <th>SCN(99)_vs_SCN(98)</th>\n",
       "      <th>DMN(40)_vs_CON(96)</th>\n",
       "      <th>DMN(17)_vs_CON(37)</th>\n",
       "      <th>DMN(17)_vs_CON(38)</th>\n",
       "      <th>IC_05</th>\n",
       "      <th>...</th>\n",
       "      <th>CON(88)_vs_ADN(56)</th>\n",
       "      <th>SMN(9)_vs_SMN(3)</th>\n",
       "      <th>SMN(11)_vs_SMN(3)</th>\n",
       "      <th>SMN(27)_vs_SMN(3)</th>\n",
       "      <th>SMN(54)_vs_SMN(3)</th>\n",
       "      <th>SMN(72)_vs_SMN(9)</th>\n",
       "      <th>CON(81)_vs_SMN(9)</th>\n",
       "      <th>CBN(13)_vs_SMN(2)</th>\n",
       "      <th>SMN(54)_vs_SMN(11)</th>\n",
       "      <th>CBN(18)_vs_SMN(11)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10001</th>\n",
       "      <td>0.005033</td>\n",
       "      <td>0.005123</td>\n",
       "      <td>0.009177</td>\n",
       "      <td>-0.013929</td>\n",
       "      <td>-0.000191</td>\n",
       "      <td>0.000769</td>\n",
       "      <td>0.000902</td>\n",
       "      <td>0.000332</td>\n",
       "      <td>-0.000460</td>\n",
       "      <td>0.004136</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000872</td>\n",
       "      <td>-0.000163</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>-0.000272</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>0.001273</td>\n",
       "      <td>0.000670</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.001074</td>\n",
       "      <td>-0.000712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10002</th>\n",
       "      <td>0.004605</td>\n",
       "      <td>0.008819</td>\n",
       "      <td>0.012004</td>\n",
       "      <td>-0.011814</td>\n",
       "      <td>-0.000067</td>\n",
       "      <td>0.001098</td>\n",
       "      <td>0.001179</td>\n",
       "      <td>0.000982</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>0.007049</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000358</td>\n",
       "      <td>0.000751</td>\n",
       "      <td>0.000433</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.000503</td>\n",
       "      <td>0.000519</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>0.000217</td>\n",
       "      <td>0.000850</td>\n",
       "      <td>0.000456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10003</th>\n",
       "      <td>0.015042</td>\n",
       "      <td>0.012548</td>\n",
       "      <td>0.018184</td>\n",
       "      <td>-0.010469</td>\n",
       "      <td>-0.000421</td>\n",
       "      <td>0.000945</td>\n",
       "      <td>0.000344</td>\n",
       "      <td>-0.000273</td>\n",
       "      <td>-0.000538</td>\n",
       "      <td>0.010444</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000495</td>\n",
       "      <td>0.000802</td>\n",
       "      <td>0.001352</td>\n",
       "      <td>0.000948</td>\n",
       "      <td>0.000879</td>\n",
       "      <td>0.000767</td>\n",
       "      <td>0.000973</td>\n",
       "      <td>-0.000993</td>\n",
       "      <td>0.001320</td>\n",
       "      <td>-0.001337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10004</th>\n",
       "      <td>0.011755</td>\n",
       "      <td>0.006837</td>\n",
       "      <td>0.005956</td>\n",
       "      <td>-0.010595</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>0.001291</td>\n",
       "      <td>0.000983</td>\n",
       "      <td>0.000622</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.006154</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000397</td>\n",
       "      <td>0.000985</td>\n",
       "      <td>0.001058</td>\n",
       "      <td>-0.000135</td>\n",
       "      <td>0.000268</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.000767</td>\n",
       "      <td>-0.000233</td>\n",
       "      <td>0.000761</td>\n",
       "      <td>-0.000555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10005</th>\n",
       "      <td>0.010679</td>\n",
       "      <td>0.005255</td>\n",
       "      <td>0.005454</td>\n",
       "      <td>-0.008591</td>\n",
       "      <td>-0.000310</td>\n",
       "      <td>0.001260</td>\n",
       "      <td>0.000916</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.009051</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.000360</td>\n",
       "      <td>0.000670</td>\n",
       "      <td>0.000589</td>\n",
       "      <td>0.000507</td>\n",
       "      <td>-0.000187</td>\n",
       "      <td>0.000473</td>\n",
       "      <td>0.000129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21750</th>\n",
       "      <td>0.020201</td>\n",
       "      <td>0.012912</td>\n",
       "      <td>0.006448</td>\n",
       "      <td>-0.007203</td>\n",
       "      <td>0.000305</td>\n",
       "      <td>0.000805</td>\n",
       "      <td>0.000581</td>\n",
       "      <td>0.000611</td>\n",
       "      <td>-0.000049</td>\n",
       "      <td>0.012353</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000287</td>\n",
       "      <td>0.000812</td>\n",
       "      <td>0.001079</td>\n",
       "      <td>0.000938</td>\n",
       "      <td>0.000722</td>\n",
       "      <td>0.001362</td>\n",
       "      <td>0.000851</td>\n",
       "      <td>-0.000728</td>\n",
       "      <td>0.001279</td>\n",
       "      <td>-0.000841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21751</th>\n",
       "      <td>0.012396</td>\n",
       "      <td>0.020112</td>\n",
       "      <td>0.012326</td>\n",
       "      <td>-0.012152</td>\n",
       "      <td>0.000243</td>\n",
       "      <td>0.001193</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>-0.000497</td>\n",
       "      <td>0.015428</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000898</td>\n",
       "      <td>0.000401</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>0.000395</td>\n",
       "      <td>0.000322</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>-0.000192</td>\n",
       "      <td>0.000711</td>\n",
       "      <td>-0.000083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21752</th>\n",
       "      <td>0.013499</td>\n",
       "      <td>0.010305</td>\n",
       "      <td>0.001160</td>\n",
       "      <td>-0.007140</td>\n",
       "      <td>-0.000354</td>\n",
       "      <td>0.001437</td>\n",
       "      <td>0.000873</td>\n",
       "      <td>0.000717</td>\n",
       "      <td>0.000305</td>\n",
       "      <td>0.010957</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000519</td>\n",
       "      <td>0.000758</td>\n",
       "      <td>0.001065</td>\n",
       "      <td>0.000951</td>\n",
       "      <td>0.000922</td>\n",
       "      <td>0.000669</td>\n",
       "      <td>0.000284</td>\n",
       "      <td>-0.000930</td>\n",
       "      <td>0.000659</td>\n",
       "      <td>-0.000723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21753</th>\n",
       "      <td>0.008602</td>\n",
       "      <td>0.017471</td>\n",
       "      <td>0.020715</td>\n",
       "      <td>-0.008130</td>\n",
       "      <td>-0.000388</td>\n",
       "      <td>0.000995</td>\n",
       "      <td>0.000960</td>\n",
       "      <td>0.000692</td>\n",
       "      <td>0.000374</td>\n",
       "      <td>0.014143</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000796</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.000814</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>0.000552</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>-0.000078</td>\n",
       "      <td>0.000527</td>\n",
       "      <td>0.000334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21754</th>\n",
       "      <td>0.006544</td>\n",
       "      <td>0.000901</td>\n",
       "      <td>0.010739</td>\n",
       "      <td>-0.007732</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.001197</td>\n",
       "      <td>0.000555</td>\n",
       "      <td>0.000295</td>\n",
       "      <td>-0.000745</td>\n",
       "      <td>0.006662</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000522</td>\n",
       "      <td>0.001129</td>\n",
       "      <td>0.001494</td>\n",
       "      <td>0.001285</td>\n",
       "      <td>0.000865</td>\n",
       "      <td>0.001023</td>\n",
       "      <td>0.000212</td>\n",
       "      <td>-0.000550</td>\n",
       "      <td>0.001013</td>\n",
       "      <td>-0.000924</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11754 rows × 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          IC_06     IC_15     IC_21     IC_28  SMN(2)_vs_SCN(53)  \\\n",
       "Id                                                                 \n",
       "10001  0.005033  0.005123  0.009177 -0.013929          -0.000191   \n",
       "10002  0.004605  0.008819  0.012004 -0.011814          -0.000067   \n",
       "10003  0.015042  0.012548  0.018184 -0.010469          -0.000421   \n",
       "10004  0.011755  0.006837  0.005956 -0.010595           0.000204   \n",
       "10005  0.010679  0.005255  0.005454 -0.008591          -0.000310   \n",
       "...         ...       ...       ...       ...                ...   \n",
       "21750  0.020201  0.012912  0.006448 -0.007203           0.000305   \n",
       "21751  0.012396  0.020112  0.012326 -0.012152           0.000243   \n",
       "21752  0.013499  0.010305  0.001160 -0.007140          -0.000354   \n",
       "21753  0.008602  0.017471  0.020715 -0.008130          -0.000388   \n",
       "21754  0.006544  0.000901  0.010739 -0.007732           0.000098   \n",
       "\n",
       "       SCN(99)_vs_SCN(98)  DMN(40)_vs_CON(96)  DMN(17)_vs_CON(37)  \\\n",
       "Id                                                                  \n",
       "10001            0.000769            0.000902            0.000332   \n",
       "10002            0.001098            0.001179            0.000982   \n",
       "10003            0.000945            0.000344           -0.000273   \n",
       "10004            0.001291            0.000983            0.000622   \n",
       "10005            0.001260            0.000916           -0.000020   \n",
       "...                   ...                 ...                 ...   \n",
       "21750            0.000805            0.000581            0.000611   \n",
       "21751            0.001193           -0.000008           -0.000020   \n",
       "21752            0.001437            0.000873            0.000717   \n",
       "21753            0.000995            0.000960            0.000692   \n",
       "21754            0.001197            0.000555            0.000295   \n",
       "\n",
       "       DMN(17)_vs_CON(38)     IC_05  ...  CON(88)_vs_ADN(56)  \\\n",
       "Id                                   ...                       \n",
       "10001           -0.000460  0.004136  ...            0.000872   \n",
       "10002           -0.000030  0.007049  ...            0.000358   \n",
       "10003           -0.000538  0.010444  ...            0.000495   \n",
       "10004            0.000154  0.006154  ...            0.000397   \n",
       "10005            0.000163  0.009051  ...           -0.000006   \n",
       "...                   ...       ...  ...                 ...   \n",
       "21750           -0.000049  0.012353  ...            0.000287   \n",
       "21751           -0.000497  0.015428  ...            0.000898   \n",
       "21752            0.000305  0.010957  ...            0.000519   \n",
       "21753            0.000374  0.014143  ...            0.000796   \n",
       "21754           -0.000745  0.006662  ...            0.000522   \n",
       "\n",
       "       SMN(9)_vs_SMN(3)  SMN(11)_vs_SMN(3)  SMN(27)_vs_SMN(3)  \\\n",
       "Id                                                              \n",
       "10001         -0.000163           0.000169          -0.000272   \n",
       "10002          0.000751           0.000433           0.000679   \n",
       "10003          0.000802           0.001352           0.000948   \n",
       "10004          0.000985           0.001058          -0.000135   \n",
       "10005          0.000261           0.000500           0.000360   \n",
       "...                 ...                ...                ...   \n",
       "21750          0.000812           0.001079           0.000938   \n",
       "21751          0.000401           0.000150           0.000395   \n",
       "21752          0.000758           0.001065           0.000951   \n",
       "21753          0.000500           0.000814           0.000227   \n",
       "21754          0.001129           0.001494           0.001285   \n",
       "\n",
       "       SMN(54)_vs_SMN(3)  SMN(72)_vs_SMN(9)  CON(81)_vs_SMN(9)  \\\n",
       "Id                                                               \n",
       "10001          -0.000002           0.001273           0.000670   \n",
       "10002           0.000503           0.000519           0.000122   \n",
       "10003           0.000879           0.000767           0.000973   \n",
       "10004           0.000268           0.001019           0.000767   \n",
       "10005           0.000670           0.000589           0.000507   \n",
       "...                  ...                ...                ...   \n",
       "21750           0.000722           0.001362           0.000851   \n",
       "21751           0.000322           0.000207           0.000026   \n",
       "21752           0.000922           0.000669           0.000284   \n",
       "21753           0.000146           0.000552           0.000107   \n",
       "21754           0.000865           0.001023           0.000212   \n",
       "\n",
       "       CBN(13)_vs_SMN(2)  SMN(54)_vs_SMN(11)  CBN(18)_vs_SMN(11)  \n",
       "Id                                                                \n",
       "10001           0.000130            0.001074           -0.000712  \n",
       "10002           0.000217            0.000850            0.000456  \n",
       "10003          -0.000993            0.001320           -0.001337  \n",
       "10004          -0.000233            0.000761           -0.000555  \n",
       "10005          -0.000187            0.000473            0.000129  \n",
       "...                  ...                 ...                 ...  \n",
       "21750          -0.000728            0.001279           -0.000841  \n",
       "21751          -0.000192            0.000711           -0.000083  \n",
       "21752          -0.000930            0.000659           -0.000723  \n",
       "21753          -0.000078            0.000527            0.000334  \n",
       "21754          -0.000550            0.001013           -0.000924  \n",
       "\n",
       "[11754 rows x 128 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full = df_full[final_columns]\n",
    "df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import nibabel as nib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import nilearn as nl\n",
    "import nibabel as nib\n",
    "# code same as pytorch data...... but took too long in this script\n",
    "# i left the code for reference\n",
    "def load_nii(file_path):\n",
    "    img = nib.load(file_path)\n",
    "    #nparray\n",
    "    data = img.get_fdata()\n",
    "    return data\n",
    "# get paths of preproccessed files\n",
    "import os\n",
    "import fnmatch\n",
    "\n",
    "BASEPATH = '/kaggle/input/'\n",
    "\n",
    "dir_list = ['pytorch-data-train-1', 'pytorch-data-train-2', 'pytorch-data-train-3', 'pytorch-data-train-4', 'pytorch-data-train-5', 'pytorch-data-train-6', 'pytorch-data-train-7',\n",
    "           'pytorch-data-test-1', 'pytorch-data-test-2', 'pytorch-data-test-3', 'pytorch-data-test-4', 'pytorch-data-test-5', 'pytorch-data-test-6', 'pytorch-data-test-7',\n",
    "           'missing-files-1', 'missing-files-2', 'missing-files-3', 'missing-files-4']\n",
    "files = {}\n",
    "for directory in dir_list:\n",
    "    for file in os.listdir(BASEPATH + directory + '/'):\n",
    "        if fnmatch.fnmatch(file, '*.npy'):\n",
    "            files[file[:-4]] = BASEPATH + directory + '/' + file\n",
    "            \n",
    "            \n",
    "dir_list = ['cf-1001fourth',\n",
    " 'cf10004fourth',\n",
    " 'cf1002fourth-test',\n",
    " 'cf1002fourth',\n",
    " 'cf1003forth',\n",
    " 'cf1004fourth-test',\n",
    " 'cf-1001fourth-test',\n",
    " 'cf1003forth-test']\n",
    "files_c = {}\n",
    "for directory in dir_list:\n",
    "    for file in os.listdir(BASEPATH + directory + '/'):\n",
    "        if fnmatch.fnmatch(file, '*.nii'):\n",
    "            files_c[file[:-4]] = BASEPATH + directory + '/' + file\n",
    "            \n",
    "            \n",
    "            \n",
    "dir_list = ['clustering-function-rena-train', 'clustering-function-rena-test']\n",
    "files_rena = {}\n",
    "for directory in dir_list:\n",
    "    for file in os.listdir(BASEPATH + directory + '/'):\n",
    "        if fnmatch.fnmatch(file, '*.nii'):\n",
    "            files_rena[file[:-4]] = BASEPATH + directory + '/' + file\n",
    "            \n",
    "dir_list = ['trends-assessment-prediction/fMRI_train', 'trends-assessment-prediction/fMRI_test']\n",
    "files_mat = {}\n",
    "for directory in dir_list:\n",
    "    for file in os.listdir(BASEPATH + directory + '/'):\n",
    "        if fnmatch.fnmatch(file, '*.mat'):\n",
    "            files_mat[file[:-4]] = BASEPATH + directory + '/' + file\n",
    "            \n",
    "# import mask image\n",
    "mask_niimg = nl.image.load_img(BASEPATH + 'trends-assessment-prediction/fMRI_mask.nii')\n",
    "\n",
    "\n",
    "class TReNDSDataset(Dataset):\n",
    "    def __init__(self, data, targets, map_path, is_train):\n",
    "        self.data = data\n",
    "        self.is_train = is_train\n",
    "        self.map_path = map_path\n",
    "        self.map_id = self.data.Id\n",
    "        if is_train: self.targets = targets\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "#         path_npy = files[str(self.map_id[idx])]\n",
    "        \n",
    "#         #features = np.load(path_npy)\n",
    "#         #features = features[:106, :, :]\n",
    "#         clusters = load_nii(files_rena[str(self.map_id[idx])])\n",
    "#         clusters = cv2.resize(np.array(clusters).transpose(1, 2, 0), (H, W))\n",
    "#         clusters = np.moveaxis(clusters, [0,1,2], [2,1,0])\n",
    "#         #features = np.concatenate((features, clusters))\n",
    "#         #features[:, 0, :] = np.array([(df_full.loc[self.map_id[idx]][:64])] * 159)\n",
    "#         #features[:, 63, :] = np.array([(df_full.loc[self.map_id[idx]][64:])] * 159)\n",
    "#         #features = np.stack([np.mean(features[::3, :, :], 0), np.mean(features[1::3, :, :], 0), np.mean(features[2::3, :, :], 0)])\n",
    "#         #standardscale based on z score, and then use pca whiten = True\n",
    "#         clusters = clusters.reshape(53, 64*64)\n",
    "#         clusters = scaler.fit_transform(clusters)\n",
    "#         clusters = clusters.reshape(53, 64, 64)\n",
    "#         features = np.concatenate([clusters, clusters, clusters])\n",
    "        path = files_mat[str(self.map_id[idx])]\n",
    "        all_maps = h5py.File(path, 'r')['SM_feature'][()]\n",
    "        all_maps = np.moveaxis(all_maps, [0,1,2,3], [3,2,1,0])\n",
    "        # load image into nifti file\n",
    "        subject_niimg = nl.image.new_img_like(mask_niimg, all_maps, affine=mask_niimg.affine, copy_header=True)\n",
    "        features = nl.image.mean_img(subject_niimg)\n",
    "        features = cv2.resize(nl.image.mean_img(subject_niimg).get_fdata().reshape(63, 52, 53).transpose(1, 2, 0), (H, W))\n",
    "        features = np.moveaxis(features, [0,1,2], [2,1,0])\n",
    "        \n",
    "        if not self.is_train:\n",
    "            return normalize(torch.FloatTensor(features))\n",
    "        else:\n",
    "            i = self.map_id[idx]\n",
    "            targets = self.targets.query('Id == {}'.format(i)).values\n",
    "            targets = np.repeat(targets[:, 1:], deep, 0).reshape(-1, 5)\n",
    "            return normalize(torch.FloatTensor(features)), torch.FloatTensor(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = files_mat['10003']\n",
    "# all_maps = h5py.File(path, 'r')['SM_feature'][()]\n",
    "# all_maps = np.moveaxis(all_maps, [0,1,2,3], [3,2,1,0])\n",
    "# # load image into nifti file\n",
    "# subject_niimg = nl.image.new_img_like(mask_niimg, all_maps, affine=mask_niimg.affine, copy_header=True)\n",
    "# features = nl.image.mean_img(subject_niimg)\n",
    "# features = cv2.resize(nl.image.mean_img(subject_niimg).get_fdata().reshape(63, 52, 53).transpose(1, 2, 0), (H, W))\n",
    "# features = np.moveaxis(features, [0,1,2], [2,1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNetModel, self).__init__()\n",
    "        \n",
    "        self.identity = lambda x: x\n",
    "        self.dense_out = nn.Linear(16, 5)\n",
    "        self.dense_in = nn.Linear(512, 16)\n",
    "        self.resnet = resnet18(pretrained=True, progress=True)\n",
    "        self.resnet = nn.Sequential(*list(self.resnet.children())[:-1])\n",
    "        \n",
    "    def forward(self, img):\n",
    "        img = img.reshape(-1, 1, H, W)\n",
    "        feat = self.resnet(img.repeat(1, 3, 1, 1))\n",
    "        conc = self.dense_in(feat.squeeze())\n",
    "        return self.identity(self.dense_out(conc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_nae(inp, targ):\n",
    "    W = torch.FloatTensor([0.3, 0.175, 0.175, 0.175, 0.175])\n",
    "    return torch.mean(torch.matmul(torch.abs(inp - targ), W.cuda()/torch.mean(targ, axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def print_metric(data, batch, epoch, start, end, metric, typ):\n",
    "    time = np.round(end - start, 1)\n",
    "    time = \"Time: %s{}%s s\".format(time)\n",
    "\n",
    "    if typ == \"Train\":\n",
    "        pre = \"BATCH %s\" + str(batch-1) + \"%s  \"\n",
    "    if typ == \"Val\":\n",
    "        pre = \"EPOCH %s\" + str(epoch+1) + \"%s  \"\n",
    "    \n",
    "    fonts = (fg(216), attr('reset'))\n",
    "    value = np.round(data.item(), 3)\n",
    "    t = typ, metric, \"%s\", value, \"%s\"\n",
    "\n",
    "    print(pre % fonts , end='')\n",
    "    print(\"{} {}: {}{}{}\".format(*t) % fonts + \"  \" + time % fonts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_out_shape = -1, 5\n",
    "train_out_shape = -1, 5\n",
    "\n",
    "split = int(SPLIT*len(train_df))\n",
    "val = train_df[split:].reset_index(drop=True)\n",
    "train = train_df[:split].reset_index(drop=True)\n",
    "\n",
    "test_set_p = TReNDSDataset(train_df, None, TRAIN_MAP_PATH, False)\n",
    "test_loader_p = DataLoader(test_set_p, batch_size=VAL_BATCH_SIZE)\n",
    "\n",
    "test_set = TReNDSDataset(test_df, None, TEST_MAP_PATH, False)\n",
    "test_loader = DataLoader(test_set, batch_size=VAL_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_resnet18():\n",
    "    def cuda(tensor):\n",
    "        return tensor.cuda()\n",
    "   \n",
    "    val_set = TReNDSDataset(val, targets, TRAIN_MAP_PATH, True)\n",
    "    val_loader = DataLoader(val_set,  batch_size=VAL_BATCH_SIZE)\n",
    "    train_set = TReNDSDataset(train, targets, TRAIN_MAP_PATH, True)\n",
    "    train_loader = DataLoader(train_set,  batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    network = cuda(ResNetModel())\n",
    "    optimizer =  Adam([{'params': network.resnet.parameters(), 'lr': LR[0]},\n",
    "                      {'params': network.dense_in.parameters(), 'lr': LR[1]},\n",
    "                      {'params': network.dense_out.parameters(), 'lr': LR[1]}])\n",
    "\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.8,\n",
    "                                  patience=4, verbose=True, eps=1e-6)\n",
    "    start = time.time()\n",
    "    for epoch in range(EPOCHS):\n",
    "        batch = 1\n",
    "        fonts = (fg(216), attr('reset'))\n",
    "        print((\"EPOCH %s\" + str(epoch+1) + \"%s\") % fonts)\n",
    "\n",
    "        for train_batch in train_loader:\n",
    "            train_img, train_targs = train_batch\n",
    "           \n",
    "            network.train()\n",
    "            network = cuda(network)\n",
    "            train_preds = network.forward(cuda(train_img))\n",
    "            train_targs = train_targs.reshape(train_out_shape)\n",
    "            train_loss = weighted_nae(train_preds, cuda(train_targs))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            train_loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            end = time.time()\n",
    "            batch = batch + 1\n",
    "            print_metric(train_loss, batch, epoch, start, end, metric=\"loss\", typ=\"Train\")\n",
    "            \n",
    "        print(\"\\n\")\n",
    "           \n",
    "        network.eval()\n",
    "        for val_batch in val_loader:\n",
    "            img, targ = val_batch\n",
    "            val_preds, val_targs = [], []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                img = cuda(img)\n",
    "                network = cuda(network)\n",
    "                pred = network.forward(img)\n",
    "                val_preds.append(pred); val_targs.append(targ)\n",
    "\n",
    "        val_preds = torch.cat(val_preds, axis=0)\n",
    "        val_targs = torch.cat(val_targs, axis=0)\n",
    "        val_targs = val_targs.reshape(val_out_shape)\n",
    "        val_loss = weighted_nae(val_preds, cuda(val_targs))\n",
    "        \n",
    "        avg_preds = []\n",
    "        avg_targs = []\n",
    "        for idx in range(0, len(val_preds), deep):\n",
    "            avg_preds.append(val_preds[idx:idx+deep].mean(axis=0))\n",
    "            avg_targs.append(val_targs[idx:idx+deep].mean(axis=0))\n",
    "            \n",
    "        avg_preds = torch.stack(avg_preds, axis=0)\n",
    "        avg_targs = torch.stack(avg_targs, axis=0)\n",
    "        loss = weighted_nae(avg_preds, cuda(avg_targs))\n",
    "        \n",
    "        end = time.time()\n",
    "        scheduler.step(val_loss)\n",
    "        print_metric(loss, None, epoch, start, end, metric=\"loss\", typ=\"Val\")\n",
    "        \n",
    "        print(\"\\n\")\n",
    "   \n",
    "    network.eval()\n",
    "    if os.path.exists(TRAIN_MAP_PATH):\n",
    "\n",
    "        test_preds = []\n",
    "        for test_img in test_loader:\n",
    "            with torch.no_grad():\n",
    "                network = cuda(network)\n",
    "                test_img = cuda(test_img)\n",
    "                test_preds.append(network.forward(test_img))\n",
    "                \n",
    "                \n",
    "                \n",
    "        test_preds_p = []\n",
    "        for test_img_p in test_loader_p:\n",
    "            with torch.no_grad():\n",
    "                network = cuda(network)\n",
    "                test_img_p = cuda(test_img_p)\n",
    "                test_preds_p.append(network.forward(test_img_p))\n",
    "        \n",
    "        \n",
    "        avg_preds = []\n",
    "        test_preds = torch.cat(test_preds, axis=0)\n",
    "        for idx in range(0, len(test_preds), deep):\n",
    "            avg_preds.append(test_preds[idx:idx+deep].mean(axis=0))\n",
    "            \n",
    "        avg_preds_p = []\n",
    "        test_preds_p = torch.cat(test_preds_p, axis=0)\n",
    "        for idx in range(0, len(test_preds_p), deep):\n",
    "            avg_preds_p.append(test_preds_p[idx:idx+deep].mean(axis=0))\n",
    "\n",
    "\n",
    "        return torch.stack(avg_preds_p, axis=0).detach().cpu().numpy(), torch.stack(avg_preds, axis=0).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING TRAINING ...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/checkpoints/resnet18-5c106cde.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "738088042b2b4656a1481f5971c0e9c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=46827520.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH \u001b[38;5;216m1\u001b[0m\n",
      "BATCH \u001b[38;5;216m1\u001b[0m  Train loss: \u001b[38;5;216m0.999\u001b[0m  Time: \u001b[38;5;216m20.9\u001b[0m s\n",
      "BATCH \u001b[38;5;216m2\u001b[0m  Train loss: \u001b[38;5;216m0.991\u001b[0m  Time: \u001b[38;5;216m37.0\u001b[0m s\n",
      "BATCH \u001b[38;5;216m3\u001b[0m  Train loss: \u001b[38;5;216m0.983\u001b[0m  Time: \u001b[38;5;216m52.6\u001b[0m s\n",
      "BATCH \u001b[38;5;216m4\u001b[0m  Train loss: \u001b[38;5;216m0.972\u001b[0m  Time: \u001b[38;5;216m67.8\u001b[0m s\n",
      "BATCH \u001b[38;5;216m5\u001b[0m  Train loss: \u001b[38;5;216m0.964\u001b[0m  Time: \u001b[38;5;216m83.2\u001b[0m s\n",
      "BATCH \u001b[38;5;216m6\u001b[0m  Train loss: \u001b[38;5;216m0.954\u001b[0m  Time: \u001b[38;5;216m101.2\u001b[0m s\n",
      "BATCH \u001b[38;5;216m7\u001b[0m  Train loss: \u001b[38;5;216m0.944\u001b[0m  Time: \u001b[38;5;216m117.0\u001b[0m s\n",
      "BATCH \u001b[38;5;216m8\u001b[0m  Train loss: \u001b[38;5;216m0.933\u001b[0m  Time: \u001b[38;5;216m133.2\u001b[0m s\n",
      "BATCH \u001b[38;5;216m9\u001b[0m  Train loss: \u001b[38;5;216m0.919\u001b[0m  Time: \u001b[38;5;216m149.6\u001b[0m s\n",
      "BATCH \u001b[38;5;216m10\u001b[0m  Train loss: \u001b[38;5;216m0.909\u001b[0m  Time: \u001b[38;5;216m165.9\u001b[0m s\n",
      "BATCH \u001b[38;5;216m11\u001b[0m  Train loss: \u001b[38;5;216m0.901\u001b[0m  Time: \u001b[38;5;216m181.9\u001b[0m s\n",
      "BATCH \u001b[38;5;216m12\u001b[0m  Train loss: \u001b[38;5;216m0.881\u001b[0m  Time: \u001b[38;5;216m197.5\u001b[0m s\n",
      "BATCH \u001b[38;5;216m13\u001b[0m  Train loss: \u001b[38;5;216m0.871\u001b[0m  Time: \u001b[38;5;216m214.3\u001b[0m s\n",
      "BATCH \u001b[38;5;216m14\u001b[0m  Train loss: \u001b[38;5;216m0.854\u001b[0m  Time: \u001b[38;5;216m231.0\u001b[0m s\n",
      "BATCH \u001b[38;5;216m15\u001b[0m  Train loss: \u001b[38;5;216m0.842\u001b[0m  Time: \u001b[38;5;216m247.2\u001b[0m s\n",
      "BATCH \u001b[38;5;216m16\u001b[0m  Train loss: \u001b[38;5;216m0.824\u001b[0m  Time: \u001b[38;5;216m263.3\u001b[0m s\n",
      "BATCH \u001b[38;5;216m17\u001b[0m  Train loss: \u001b[38;5;216m0.807\u001b[0m  Time: \u001b[38;5;216m280.1\u001b[0m s\n",
      "BATCH \u001b[38;5;216m18\u001b[0m  Train loss: \u001b[38;5;216m0.794\u001b[0m  Time: \u001b[38;5;216m295.8\u001b[0m s\n",
      "BATCH \u001b[38;5;216m19\u001b[0m  Train loss: \u001b[38;5;216m0.769\u001b[0m  Time: \u001b[38;5;216m311.4\u001b[0m s\n",
      "BATCH \u001b[38;5;216m20\u001b[0m  Train loss: \u001b[38;5;216m0.759\u001b[0m  Time: \u001b[38;5;216m328.1\u001b[0m s\n",
      "BATCH \u001b[38;5;216m21\u001b[0m  Train loss: \u001b[38;5;216m0.723\u001b[0m  Time: \u001b[38;5;216m344.0\u001b[0m s\n",
      "BATCH \u001b[38;5;216m22\u001b[0m  Train loss: \u001b[38;5;216m0.717\u001b[0m  Time: \u001b[38;5;216m359.5\u001b[0m s\n",
      "BATCH \u001b[38;5;216m23\u001b[0m  Train loss: \u001b[38;5;216m0.693\u001b[0m  Time: \u001b[38;5;216m375.5\u001b[0m s\n",
      "BATCH \u001b[38;5;216m24\u001b[0m  Train loss: \u001b[38;5;216m0.678\u001b[0m  Time: \u001b[38;5;216m392.3\u001b[0m s\n",
      "BATCH \u001b[38;5;216m25\u001b[0m  Train loss: \u001b[38;5;216m0.652\u001b[0m  Time: \u001b[38;5;216m408.5\u001b[0m s\n",
      "BATCH \u001b[38;5;216m26\u001b[0m  Train loss: \u001b[38;5;216m0.63\u001b[0m  Time: \u001b[38;5;216m424.2\u001b[0m s\n",
      "BATCH \u001b[38;5;216m27\u001b[0m  Train loss: \u001b[38;5;216m0.621\u001b[0m  Time: \u001b[38;5;216m440.4\u001b[0m s\n",
      "BATCH \u001b[38;5;216m28\u001b[0m  Train loss: \u001b[38;5;216m0.586\u001b[0m  Time: \u001b[38;5;216m457.1\u001b[0m s\n",
      "BATCH \u001b[38;5;216m29\u001b[0m  Train loss: \u001b[38;5;216m0.561\u001b[0m  Time: \u001b[38;5;216m474.0\u001b[0m s\n",
      "BATCH \u001b[38;5;216m30\u001b[0m  Train loss: \u001b[38;5;216m0.554\u001b[0m  Time: \u001b[38;5;216m489.8\u001b[0m s\n",
      "BATCH \u001b[38;5;216m31\u001b[0m  Train loss: \u001b[38;5;216m0.523\u001b[0m  Time: \u001b[38;5;216m506.6\u001b[0m s\n",
      "BATCH \u001b[38;5;216m32\u001b[0m  Train loss: \u001b[38;5;216m0.493\u001b[0m  Time: \u001b[38;5;216m522.5\u001b[0m s\n",
      "BATCH \u001b[38;5;216m33\u001b[0m  Train loss: \u001b[38;5;216m0.47\u001b[0m  Time: \u001b[38;5;216m538.6\u001b[0m s\n",
      "BATCH \u001b[38;5;216m34\u001b[0m  Train loss: \u001b[38;5;216m0.454\u001b[0m  Time: \u001b[38;5;216m554.1\u001b[0m s\n",
      "BATCH \u001b[38;5;216m35\u001b[0m  Train loss: \u001b[38;5;216m0.427\u001b[0m  Time: \u001b[38;5;216m570.7\u001b[0m s\n",
      "BATCH \u001b[38;5;216m36\u001b[0m  Train loss: \u001b[38;5;216m0.399\u001b[0m  Time: \u001b[38;5;216m586.4\u001b[0m s\n",
      "BATCH \u001b[38;5;216m37\u001b[0m  Train loss: \u001b[38;5;216m0.381\u001b[0m  Time: \u001b[38;5;216m602.2\u001b[0m s\n",
      "BATCH \u001b[38;5;216m38\u001b[0m  Train loss: \u001b[38;5;216m0.365\u001b[0m  Time: \u001b[38;5;216m618.0\u001b[0m s\n",
      "BATCH \u001b[38;5;216m39\u001b[0m  Train loss: \u001b[38;5;216m0.349\u001b[0m  Time: \u001b[38;5;216m634.7\u001b[0m s\n",
      "BATCH \u001b[38;5;216m40\u001b[0m  Train loss: \u001b[38;5;216m0.345\u001b[0m  Time: \u001b[38;5;216m650.2\u001b[0m s\n",
      "BATCH \u001b[38;5;216m41\u001b[0m  Train loss: \u001b[38;5;216m0.306\u001b[0m  Time: \u001b[38;5;216m666.4\u001b[0m s\n",
      "BATCH \u001b[38;5;216m42\u001b[0m  Train loss: \u001b[38;5;216m0.298\u001b[0m  Time: \u001b[38;5;216m682.4\u001b[0m s\n",
      "BATCH \u001b[38;5;216m43\u001b[0m  Train loss: \u001b[38;5;216m0.281\u001b[0m  Time: \u001b[38;5;216m699.2\u001b[0m s\n",
      "BATCH \u001b[38;5;216m44\u001b[0m  Train loss: \u001b[38;5;216m0.258\u001b[0m  Time: \u001b[38;5;216m715.0\u001b[0m s\n",
      "BATCH \u001b[38;5;216m45\u001b[0m  Train loss: \u001b[38;5;216m0.24\u001b[0m  Time: \u001b[38;5;216m730.9\u001b[0m s\n",
      "BATCH \u001b[38;5;216m46\u001b[0m  Train loss: \u001b[38;5;216m0.239\u001b[0m  Time: \u001b[38;5;216m747.5\u001b[0m s\n",
      "BATCH \u001b[38;5;216m47\u001b[0m  Train loss: \u001b[38;5;216m0.218\u001b[0m  Time: \u001b[38;5;216m764.0\u001b[0m s\n",
      "BATCH \u001b[38;5;216m48\u001b[0m  Train loss: \u001b[38;5;216m0.198\u001b[0m  Time: \u001b[38;5;216m780.1\u001b[0m s\n",
      "BATCH \u001b[38;5;216m49\u001b[0m  Train loss: \u001b[38;5;216m0.205\u001b[0m  Time: \u001b[38;5;216m796.1\u001b[0m s\n",
      "BATCH \u001b[38;5;216m50\u001b[0m  Train loss: \u001b[38;5;216m0.201\u001b[0m  Time: \u001b[38;5;216m812.7\u001b[0m s\n",
      "BATCH \u001b[38;5;216m51\u001b[0m  Train loss: \u001b[38;5;216m0.175\u001b[0m  Time: \u001b[38;5;216m828.4\u001b[0m s\n",
      "BATCH \u001b[38;5;216m52\u001b[0m  Train loss: \u001b[38;5;216m0.194\u001b[0m  Time: \u001b[38;5;216m843.8\u001b[0m s\n",
      "BATCH \u001b[38;5;216m53\u001b[0m  Train loss: \u001b[38;5;216m0.205\u001b[0m  Time: \u001b[38;5;216m859.4\u001b[0m s\n",
      "BATCH \u001b[38;5;216m54\u001b[0m  Train loss: \u001b[38;5;216m0.187\u001b[0m  Time: \u001b[38;5;216m875.8\u001b[0m s\n",
      "BATCH \u001b[38;5;216m55\u001b[0m  Train loss: \u001b[38;5;216m0.2\u001b[0m  Time: \u001b[38;5;216m891.6\u001b[0m s\n",
      "BATCH \u001b[38;5;216m56\u001b[0m  Train loss: \u001b[38;5;216m0.189\u001b[0m  Time: \u001b[38;5;216m907.2\u001b[0m s\n",
      "BATCH \u001b[38;5;216m57\u001b[0m  Train loss: \u001b[38;5;216m0.196\u001b[0m  Time: \u001b[38;5;216m922.9\u001b[0m s\n",
      "BATCH \u001b[38;5;216m58\u001b[0m  Train loss: \u001b[38;5;216m0.18\u001b[0m  Time: \u001b[38;5;216m939.3\u001b[0m s\n",
      "BATCH \u001b[38;5;216m59\u001b[0m  Train loss: \u001b[38;5;216m0.202\u001b[0m  Time: \u001b[38;5;216m955.0\u001b[0m s\n",
      "BATCH \u001b[38;5;216m60\u001b[0m  Train loss: \u001b[38;5;216m0.185\u001b[0m  Time: \u001b[38;5;216m970.4\u001b[0m s\n",
      "BATCH \u001b[38;5;216m61\u001b[0m  Train loss: \u001b[38;5;216m0.217\u001b[0m  Time: \u001b[38;5;216m987.3\u001b[0m s\n",
      "BATCH \u001b[38;5;216m62\u001b[0m  Train loss: \u001b[38;5;216m0.174\u001b[0m  Time: \u001b[38;5;216m1003.9\u001b[0m s\n",
      "BATCH \u001b[38;5;216m63\u001b[0m  Train loss: \u001b[38;5;216m0.201\u001b[0m  Time: \u001b[38;5;216m1020.5\u001b[0m s\n",
      "BATCH \u001b[38;5;216m64\u001b[0m  Train loss: \u001b[38;5;216m0.201\u001b[0m  Time: \u001b[38;5;216m1036.7\u001b[0m s\n",
      "BATCH \u001b[38;5;216m65\u001b[0m  Train loss: \u001b[38;5;216m0.169\u001b[0m  Time: \u001b[38;5;216m1053.8\u001b[0m s\n",
      "BATCH \u001b[38;5;216m66\u001b[0m  Train loss: \u001b[38;5;216m0.173\u001b[0m  Time: \u001b[38;5;216m1070.2\u001b[0m s\n",
      "BATCH \u001b[38;5;216m67\u001b[0m  Train loss: \u001b[38;5;216m0.179\u001b[0m  Time: \u001b[38;5;216m1086.0\u001b[0m s\n",
      "BATCH \u001b[38;5;216m68\u001b[0m  Train loss: \u001b[38;5;216m0.17\u001b[0m  Time: \u001b[38;5;216m1102.3\u001b[0m s\n",
      "BATCH \u001b[38;5;216m69\u001b[0m  Train loss: \u001b[38;5;216m0.17\u001b[0m  Time: \u001b[38;5;216m1120.4\u001b[0m s\n",
      "BATCH \u001b[38;5;216m70\u001b[0m  Train loss: \u001b[38;5;216m0.161\u001b[0m  Time: \u001b[38;5;216m1136.4\u001b[0m s\n",
      "BATCH \u001b[38;5;216m71\u001b[0m  Train loss: \u001b[38;5;216m0.193\u001b[0m  Time: \u001b[38;5;216m1153.1\u001b[0m s\n",
      "BATCH \u001b[38;5;216m72\u001b[0m  Train loss: \u001b[38;5;216m0.199\u001b[0m  Time: \u001b[38;5;216m1169.6\u001b[0m s\n",
      "BATCH \u001b[38;5;216m73\u001b[0m  Train loss: \u001b[38;5;216m0.166\u001b[0m  Time: \u001b[38;5;216m1185.8\u001b[0m s\n",
      "BATCH \u001b[38;5;216m74\u001b[0m  Train loss: \u001b[38;5;216m0.213\u001b[0m  Time: \u001b[38;5;216m1201.9\u001b[0m s\n",
      "BATCH \u001b[38;5;216m75\u001b[0m  Train loss: \u001b[38;5;216m0.172\u001b[0m  Time: \u001b[38;5;216m1217.8\u001b[0m s\n",
      "BATCH \u001b[38;5;216m76\u001b[0m  Train loss: \u001b[38;5;216m0.171\u001b[0m  Time: \u001b[38;5;216m1234.5\u001b[0m s\n",
      "BATCH \u001b[38;5;216m77\u001b[0m  Train loss: \u001b[38;5;216m0.18\u001b[0m  Time: \u001b[38;5;216m1251.5\u001b[0m s\n",
      "BATCH \u001b[38;5;216m78\u001b[0m  Train loss: \u001b[38;5;216m0.184\u001b[0m  Time: \u001b[38;5;216m1267.4\u001b[0m s\n",
      "BATCH \u001b[38;5;216m79\u001b[0m  Train loss: \u001b[38;5;216m0.164\u001b[0m  Time: \u001b[38;5;216m1283.3\u001b[0m s\n",
      "BATCH \u001b[38;5;216m80\u001b[0m  Train loss: \u001b[38;5;216m0.178\u001b[0m  Time: \u001b[38;5;216m1299.9\u001b[0m s\n",
      "BATCH \u001b[38;5;216m81\u001b[0m  Train loss: \u001b[38;5;216m0.175\u001b[0m  Time: \u001b[38;5;216m1315.7\u001b[0m s\n",
      "BATCH \u001b[38;5;216m82\u001b[0m  Train loss: \u001b[38;5;216m0.207\u001b[0m  Time: \u001b[38;5;216m1332.2\u001b[0m s\n",
      "BATCH \u001b[38;5;216m83\u001b[0m  Train loss: \u001b[38;5;216m0.174\u001b[0m  Time: \u001b[38;5;216m1348.8\u001b[0m s\n",
      "BATCH \u001b[38;5;216m84\u001b[0m  Train loss: \u001b[38;5;216m0.205\u001b[0m  Time: \u001b[38;5;216m1364.9\u001b[0m s\n",
      "BATCH \u001b[38;5;216m85\u001b[0m  Train loss: \u001b[38;5;216m0.187\u001b[0m  Time: \u001b[38;5;216m1381.2\u001b[0m s\n",
      "BATCH \u001b[38;5;216m86\u001b[0m  Train loss: \u001b[38;5;216m0.191\u001b[0m  Time: \u001b[38;5;216m1397.3\u001b[0m s\n",
      "BATCH \u001b[38;5;216m87\u001b[0m  Train loss: \u001b[38;5;216m0.185\u001b[0m  Time: \u001b[38;5;216m1415.3\u001b[0m s\n",
      "BATCH \u001b[38;5;216m88\u001b[0m  Train loss: \u001b[38;5;216m0.187\u001b[0m  Time: \u001b[38;5;216m1431.0\u001b[0m s\n",
      "BATCH \u001b[38;5;216m89\u001b[0m  Train loss: \u001b[38;5;216m0.194\u001b[0m  Time: \u001b[38;5;216m1446.4\u001b[0m s\n",
      "BATCH \u001b[38;5;216m90\u001b[0m  Train loss: \u001b[38;5;216m0.189\u001b[0m  Time: \u001b[38;5;216m1462.1\u001b[0m s\n",
      "BATCH \u001b[38;5;216m91\u001b[0m  Train loss: \u001b[38;5;216m0.19\u001b[0m  Time: \u001b[38;5;216m1478.5\u001b[0m s\n",
      "BATCH \u001b[38;5;216m92\u001b[0m  Train loss: \u001b[38;5;216m0.141\u001b[0m  Time: \u001b[38;5;216m1494.6\u001b[0m s\n",
      "BATCH \u001b[38;5;216m93\u001b[0m  Train loss: \u001b[38;5;216m0.194\u001b[0m  Time: \u001b[38;5;216m1510.0\u001b[0m s\n",
      "BATCH \u001b[38;5;216m94\u001b[0m  Train loss: \u001b[38;5;216m0.181\u001b[0m  Time: \u001b[38;5;216m1526.6\u001b[0m s\n",
      "BATCH \u001b[38;5;216m95\u001b[0m  Train loss: \u001b[38;5;216m0.181\u001b[0m  Time: \u001b[38;5;216m1542.2\u001b[0m s\n",
      "BATCH \u001b[38;5;216m96\u001b[0m  Train loss: \u001b[38;5;216m0.16\u001b[0m  Time: \u001b[38;5;216m1558.4\u001b[0m s\n",
      "BATCH \u001b[38;5;216m97\u001b[0m  Train loss: \u001b[38;5;216m0.181\u001b[0m  Time: \u001b[38;5;216m1574.1\u001b[0m s\n",
      "BATCH \u001b[38;5;216m98\u001b[0m  Train loss: \u001b[38;5;216m0.17\u001b[0m  Time: \u001b[38;5;216m1590.8\u001b[0m s\n",
      "BATCH \u001b[38;5;216m99\u001b[0m  Train loss: \u001b[38;5;216m0.168\u001b[0m  Time: \u001b[38;5;216m1607.0\u001b[0m s\n",
      "BATCH \u001b[38;5;216m100\u001b[0m  Train loss: \u001b[38;5;216m0.176\u001b[0m  Time: \u001b[38;5;216m1622.4\u001b[0m s\n",
      "BATCH \u001b[38;5;216m101\u001b[0m  Train loss: \u001b[38;5;216m0.171\u001b[0m  Time: \u001b[38;5;216m1638.5\u001b[0m s\n",
      "BATCH \u001b[38;5;216m102\u001b[0m  Train loss: \u001b[38;5;216m0.165\u001b[0m  Time: \u001b[38;5;216m1655.0\u001b[0m s\n",
      "BATCH \u001b[38;5;216m103\u001b[0m  Train loss: \u001b[38;5;216m0.186\u001b[0m  Time: \u001b[38;5;216m1670.9\u001b[0m s\n",
      "BATCH \u001b[38;5;216m104\u001b[0m  Train loss: \u001b[38;5;216m0.193\u001b[0m  Time: \u001b[38;5;216m1686.6\u001b[0m s\n",
      "BATCH \u001b[38;5;216m105\u001b[0m  Train loss: \u001b[38;5;216m0.165\u001b[0m  Time: \u001b[38;5;216m1702.6\u001b[0m s\n",
      "BATCH \u001b[38;5;216m106\u001b[0m  Train loss: \u001b[38;5;216m0.173\u001b[0m  Time: \u001b[38;5;216m1719.3\u001b[0m s\n",
      "BATCH \u001b[38;5;216m107\u001b[0m  Train loss: \u001b[38;5;216m0.183\u001b[0m  Time: \u001b[38;5;216m1735.2\u001b[0m s\n",
      "BATCH \u001b[38;5;216m108\u001b[0m  Train loss: \u001b[38;5;216m0.192\u001b[0m  Time: \u001b[38;5;216m1750.6\u001b[0m s\n",
      "BATCH \u001b[38;5;216m109\u001b[0m  Train loss: \u001b[38;5;216m0.157\u001b[0m  Time: \u001b[38;5;216m1767.2\u001b[0m s\n",
      "BATCH \u001b[38;5;216m110\u001b[0m  Train loss: \u001b[38;5;216m0.178\u001b[0m  Time: \u001b[38;5;216m1783.6\u001b[0m s\n",
      "BATCH \u001b[38;5;216m111\u001b[0m  Train loss: \u001b[38;5;216m0.169\u001b[0m  Time: \u001b[38;5;216m1800.4\u001b[0m s\n",
      "BATCH \u001b[38;5;216m112\u001b[0m  Train loss: \u001b[38;5;216m0.206\u001b[0m  Time: \u001b[38;5;216m1816.6\u001b[0m s\n",
      "BATCH \u001b[38;5;216m113\u001b[0m  Train loss: \u001b[38;5;216m0.192\u001b[0m  Time: \u001b[38;5;216m1833.6\u001b[0m s\n",
      "BATCH \u001b[38;5;216m114\u001b[0m  Train loss: \u001b[38;5;216m0.182\u001b[0m  Time: \u001b[38;5;216m1850.4\u001b[0m s\n",
      "BATCH \u001b[38;5;216m115\u001b[0m  Train loss: \u001b[38;5;216m0.178\u001b[0m  Time: \u001b[38;5;216m1866.5\u001b[0m s\n",
      "BATCH \u001b[38;5;216m116\u001b[0m  Train loss: \u001b[38;5;216m0.168\u001b[0m  Time: \u001b[38;5;216m1882.7\u001b[0m s\n",
      "BATCH \u001b[38;5;216m117\u001b[0m  Train loss: \u001b[38;5;216m0.192\u001b[0m  Time: \u001b[38;5;216m1899.7\u001b[0m s\n",
      "BATCH \u001b[38;5;216m118\u001b[0m  Train loss: \u001b[38;5;216m0.167\u001b[0m  Time: \u001b[38;5;216m1915.7\u001b[0m s\n",
      "BATCH \u001b[38;5;216m119\u001b[0m  Train loss: \u001b[38;5;216m0.175\u001b[0m  Time: \u001b[38;5;216m1931.7\u001b[0m s\n",
      "BATCH \u001b[38;5;216m120\u001b[0m  Train loss: \u001b[38;5;216m0.222\u001b[0m  Time: \u001b[38;5;216m1948.8\u001b[0m s\n",
      "BATCH \u001b[38;5;216m121\u001b[0m  Train loss: \u001b[38;5;216m0.218\u001b[0m  Time: \u001b[38;5;216m1964.8\u001b[0m s\n",
      "BATCH \u001b[38;5;216m122\u001b[0m  Train loss: \u001b[38;5;216m0.182\u001b[0m  Time: \u001b[38;5;216m1981.4\u001b[0m s\n",
      "BATCH \u001b[38;5;216m123\u001b[0m  Train loss: \u001b[38;5;216m0.168\u001b[0m  Time: \u001b[38;5;216m1997.4\u001b[0m s\n",
      "BATCH \u001b[38;5;216m124\u001b[0m  Train loss: \u001b[38;5;216m0.181\u001b[0m  Time: \u001b[38;5;216m2014.4\u001b[0m s\n",
      "BATCH \u001b[38;5;216m125\u001b[0m  Train loss: \u001b[38;5;216m0.168\u001b[0m  Time: \u001b[38;5;216m2030.4\u001b[0m s\n",
      "BATCH \u001b[38;5;216m126\u001b[0m  Train loss: \u001b[38;5;216m0.176\u001b[0m  Time: \u001b[38;5;216m2046.5\u001b[0m s\n",
      "BATCH \u001b[38;5;216m127\u001b[0m  Train loss: \u001b[38;5;216m0.175\u001b[0m  Time: \u001b[38;5;216m2062.2\u001b[0m s\n",
      "BATCH \u001b[38;5;216m128\u001b[0m  Train loss: \u001b[38;5;216m0.172\u001b[0m  Time: \u001b[38;5;216m2079.3\u001b[0m s\n",
      "BATCH \u001b[38;5;216m129\u001b[0m  Train loss: \u001b[38;5;216m0.164\u001b[0m  Time: \u001b[38;5;216m2095.0\u001b[0m s\n",
      "BATCH \u001b[38;5;216m130\u001b[0m  Train loss: \u001b[38;5;216m0.196\u001b[0m  Time: \u001b[38;5;216m2111.2\u001b[0m s\n",
      "BATCH \u001b[38;5;216m131\u001b[0m  Train loss: \u001b[38;5;216m0.159\u001b[0m  Time: \u001b[38;5;216m2127.6\u001b[0m s\n",
      "BATCH \u001b[38;5;216m132\u001b[0m  Train loss: \u001b[38;5;216m0.183\u001b[0m  Time: \u001b[38;5;216m2144.2\u001b[0m s\n",
      "BATCH \u001b[38;5;216m133\u001b[0m  Train loss: \u001b[38;5;216m0.181\u001b[0m  Time: \u001b[38;5;216m2159.9\u001b[0m s\n",
      "BATCH \u001b[38;5;216m134\u001b[0m  Train loss: \u001b[38;5;216m0.172\u001b[0m  Time: \u001b[38;5;216m2175.9\u001b[0m s\n",
      "BATCH \u001b[38;5;216m135\u001b[0m  Train loss: \u001b[38;5;216m0.188\u001b[0m  Time: \u001b[38;5;216m2192.7\u001b[0m s\n",
      "BATCH \u001b[38;5;216m136\u001b[0m  Train loss: \u001b[38;5;216m0.174\u001b[0m  Time: \u001b[38;5;216m2209.0\u001b[0m s\n",
      "BATCH \u001b[38;5;216m137\u001b[0m  Train loss: \u001b[38;5;216m0.183\u001b[0m  Time: \u001b[38;5;216m2224.9\u001b[0m s\n",
      "BATCH \u001b[38;5;216m138\u001b[0m  Train loss: \u001b[38;5;216m0.2\u001b[0m  Time: \u001b[38;5;216m2241.0\u001b[0m s\n",
      "BATCH \u001b[38;5;216m139\u001b[0m  Train loss: \u001b[38;5;216m0.191\u001b[0m  Time: \u001b[38;5;216m2258.4\u001b[0m s\n",
      "BATCH \u001b[38;5;216m140\u001b[0m  Train loss: \u001b[38;5;216m0.178\u001b[0m  Time: \u001b[38;5;216m2274.6\u001b[0m s\n",
      "BATCH \u001b[38;5;216m141\u001b[0m  Train loss: \u001b[38;5;216m0.19\u001b[0m  Time: \u001b[38;5;216m2290.5\u001b[0m s\n",
      "BATCH \u001b[38;5;216m142\u001b[0m  Train loss: \u001b[38;5;216m0.208\u001b[0m  Time: \u001b[38;5;216m2307.9\u001b[0m s\n",
      "BATCH \u001b[38;5;216m143\u001b[0m  Train loss: \u001b[38;5;216m0.169\u001b[0m  Time: \u001b[38;5;216m2323.8\u001b[0m s\n",
      "BATCH \u001b[38;5;216m144\u001b[0m  Train loss: \u001b[38;5;216m0.179\u001b[0m  Time: \u001b[38;5;216m2340.2\u001b[0m s\n",
      "BATCH \u001b[38;5;216m145\u001b[0m  Train loss: \u001b[38;5;216m0.186\u001b[0m  Time: \u001b[38;5;216m2356.3\u001b[0m s\n",
      "BATCH \u001b[38;5;216m146\u001b[0m  Train loss: \u001b[38;5;216m0.177\u001b[0m  Time: \u001b[38;5;216m2374.5\u001b[0m s\n",
      "BATCH \u001b[38;5;216m147\u001b[0m  Train loss: \u001b[38;5;216m0.184\u001b[0m  Time: \u001b[38;5;216m2393.9\u001b[0m s\n",
      "\n",
      "\n",
      "EPOCH \u001b[38;5;216m1\u001b[0m  Val loss: \u001b[38;5;216m0.181\u001b[0m  Time: \u001b[38;5;216m2979.2\u001b[0m s\n",
      "\n",
      "\n",
      "EPOCH \u001b[38;5;216m2\u001b[0m\n",
      "BATCH \u001b[38;5;216m1\u001b[0m  Train loss: \u001b[38;5;216m0.18\u001b[0m  Time: \u001b[38;5;216m2994.4\u001b[0m s\n",
      "BATCH \u001b[38;5;216m2\u001b[0m  Train loss: \u001b[38;5;216m0.186\u001b[0m  Time: \u001b[38;5;216m3009.9\u001b[0m s\n",
      "BATCH \u001b[38;5;216m3\u001b[0m  Train loss: \u001b[38;5;216m0.181\u001b[0m  Time: \u001b[38;5;216m3026.4\u001b[0m s\n",
      "BATCH \u001b[38;5;216m4\u001b[0m  Train loss: \u001b[38;5;216m0.177\u001b[0m  Time: \u001b[38;5;216m3042.8\u001b[0m s\n",
      "BATCH \u001b[38;5;216m5\u001b[0m  Train loss: \u001b[38;5;216m0.192\u001b[0m  Time: \u001b[38;5;216m3059.5\u001b[0m s\n",
      "BATCH \u001b[38;5;216m6\u001b[0m  Train loss: \u001b[38;5;216m0.175\u001b[0m  Time: \u001b[38;5;216m3076.1\u001b[0m s\n",
      "BATCH \u001b[38;5;216m7\u001b[0m  Train loss: \u001b[38;5;216m0.181\u001b[0m  Time: \u001b[38;5;216m3094.3\u001b[0m s\n",
      "BATCH \u001b[38;5;216m8\u001b[0m  Train loss: \u001b[38;5;216m0.164\u001b[0m  Time: \u001b[38;5;216m3110.7\u001b[0m s\n",
      "BATCH \u001b[38;5;216m9\u001b[0m  Train loss: \u001b[38;5;216m0.191\u001b[0m  Time: \u001b[38;5;216m3126.3\u001b[0m s\n",
      "BATCH \u001b[38;5;216m10\u001b[0m  Train loss: \u001b[38;5;216m0.183\u001b[0m  Time: \u001b[38;5;216m3142.1\u001b[0m s\n",
      "BATCH \u001b[38;5;216m11\u001b[0m  Train loss: \u001b[38;5;216m0.189\u001b[0m  Time: \u001b[38;5;216m3159.0\u001b[0m s\n",
      "BATCH \u001b[38;5;216m12\u001b[0m  Train loss: \u001b[38;5;216m0.199\u001b[0m  Time: \u001b[38;5;216m3174.6\u001b[0m s\n",
      "BATCH \u001b[38;5;216m13\u001b[0m  Train loss: \u001b[38;5;216m0.178\u001b[0m  Time: \u001b[38;5;216m3190.5\u001b[0m s\n",
      "BATCH \u001b[38;5;216m14\u001b[0m  Train loss: \u001b[38;5;216m0.179\u001b[0m  Time: \u001b[38;5;216m3206.8\u001b[0m s\n",
      "BATCH \u001b[38;5;216m15\u001b[0m  Train loss: \u001b[38;5;216m0.181\u001b[0m  Time: \u001b[38;5;216m3223.1\u001b[0m s\n",
      "BATCH \u001b[38;5;216m16\u001b[0m  Train loss: \u001b[38;5;216m0.178\u001b[0m  Time: \u001b[38;5;216m3238.5\u001b[0m s\n",
      "BATCH \u001b[38;5;216m17\u001b[0m  Train loss: \u001b[38;5;216m0.204\u001b[0m  Time: \u001b[38;5;216m3255.5\u001b[0m s\n",
      "BATCH \u001b[38;5;216m18\u001b[0m  Train loss: \u001b[38;5;216m0.181\u001b[0m  Time: \u001b[38;5;216m3272.5\u001b[0m s\n",
      "BATCH \u001b[38;5;216m19\u001b[0m  Train loss: \u001b[38;5;216m0.199\u001b[0m  Time: \u001b[38;5;216m3288.4\u001b[0m s\n",
      "BATCH \u001b[38;5;216m20\u001b[0m  Train loss: \u001b[38;5;216m0.172\u001b[0m  Time: \u001b[38;5;216m3304.6\u001b[0m s\n",
      "BATCH \u001b[38;5;216m21\u001b[0m  Train loss: \u001b[38;5;216m0.169\u001b[0m  Time: \u001b[38;5;216m3320.4\u001b[0m s\n",
      "BATCH \u001b[38;5;216m22\u001b[0m  Train loss: \u001b[38;5;216m0.178\u001b[0m  Time: \u001b[38;5;216m3337.5\u001b[0m s\n",
      "BATCH \u001b[38;5;216m23\u001b[0m  Train loss: \u001b[38;5;216m0.2\u001b[0m  Time: \u001b[38;5;216m3354.6\u001b[0m s\n",
      "BATCH \u001b[38;5;216m24\u001b[0m  Train loss: \u001b[38;5;216m0.171\u001b[0m  Time: \u001b[38;5;216m3371.3\u001b[0m s\n",
      "BATCH \u001b[38;5;216m25\u001b[0m  Train loss: \u001b[38;5;216m0.169\u001b[0m  Time: \u001b[38;5;216m3388.0\u001b[0m s\n",
      "BATCH \u001b[38;5;216m26\u001b[0m  Train loss: \u001b[38;5;216m0.18\u001b[0m  Time: \u001b[38;5;216m3403.8\u001b[0m s\n",
      "BATCH \u001b[38;5;216m27\u001b[0m  Train loss: \u001b[38;5;216m0.197\u001b[0m  Time: \u001b[38;5;216m3420.2\u001b[0m s\n",
      "BATCH \u001b[38;5;216m28\u001b[0m  Train loss: \u001b[38;5;216m0.187\u001b[0m  Time: \u001b[38;5;216m3436.1\u001b[0m s\n",
      "BATCH \u001b[38;5;216m29\u001b[0m  Train loss: \u001b[38;5;216m0.145\u001b[0m  Time: \u001b[38;5;216m3452.9\u001b[0m s\n",
      "BATCH \u001b[38;5;216m30\u001b[0m  Train loss: \u001b[38;5;216m0.167\u001b[0m  Time: \u001b[38;5;216m3468.4\u001b[0m s\n",
      "BATCH \u001b[38;5;216m31\u001b[0m  Train loss: \u001b[38;5;216m0.152\u001b[0m  Time: \u001b[38;5;216m3484.1\u001b[0m s\n",
      "BATCH \u001b[38;5;216m32\u001b[0m  Train loss: \u001b[38;5;216m0.169\u001b[0m  Time: \u001b[38;5;216m3499.7\u001b[0m s\n",
      "BATCH \u001b[38;5;216m33\u001b[0m  Train loss: \u001b[38;5;216m0.16\u001b[0m  Time: \u001b[38;5;216m3516.8\u001b[0m s\n",
      "BATCH \u001b[38;5;216m34\u001b[0m  Train loss: \u001b[38;5;216m0.178\u001b[0m  Time: \u001b[38;5;216m3532.6\u001b[0m s\n",
      "BATCH \u001b[38;5;216m35\u001b[0m  Train loss: \u001b[38;5;216m0.2\u001b[0m  Time: \u001b[38;5;216m3548.1\u001b[0m s\n",
      "BATCH \u001b[38;5;216m36\u001b[0m  Train loss: \u001b[38;5;216m0.199\u001b[0m  Time: \u001b[38;5;216m3563.8\u001b[0m s\n",
      "BATCH \u001b[38;5;216m37\u001b[0m  Train loss: \u001b[38;5;216m0.191\u001b[0m  Time: \u001b[38;5;216m3580.9\u001b[0m s\n",
      "BATCH \u001b[38;5;216m38\u001b[0m  Train loss: \u001b[38;5;216m0.147\u001b[0m  Time: \u001b[38;5;216m3596.7\u001b[0m s\n",
      "BATCH \u001b[38;5;216m39\u001b[0m  Train loss: \u001b[38;5;216m0.176\u001b[0m  Time: \u001b[38;5;216m3612.1\u001b[0m s\n",
      "BATCH \u001b[38;5;216m40\u001b[0m  Train loss: \u001b[38;5;216m0.187\u001b[0m  Time: \u001b[38;5;216m3628.4\u001b[0m s\n",
      "BATCH \u001b[38;5;216m41\u001b[0m  Train loss: \u001b[38;5;216m0.16\u001b[0m  Time: \u001b[38;5;216m3644.3\u001b[0m s\n",
      "BATCH \u001b[38;5;216m42\u001b[0m  Train loss: \u001b[38;5;216m0.187\u001b[0m  Time: \u001b[38;5;216m3659.6\u001b[0m s\n",
      "BATCH \u001b[38;5;216m43\u001b[0m  Train loss: \u001b[38;5;216m0.171\u001b[0m  Time: \u001b[38;5;216m3675.2\u001b[0m s\n",
      "BATCH \u001b[38;5;216m44\u001b[0m  Train loss: \u001b[38;5;216m0.205\u001b[0m  Time: \u001b[38;5;216m3691.6\u001b[0m s\n",
      "BATCH \u001b[38;5;216m45\u001b[0m  Train loss: \u001b[38;5;216m0.18\u001b[0m  Time: \u001b[38;5;216m3707.4\u001b[0m s\n",
      "BATCH \u001b[38;5;216m46\u001b[0m  Train loss: \u001b[38;5;216m0.196\u001b[0m  Time: \u001b[38;5;216m3722.8\u001b[0m s\n",
      "BATCH \u001b[38;5;216m47\u001b[0m  Train loss: \u001b[38;5;216m0.199\u001b[0m  Time: \u001b[38;5;216m3738.6\u001b[0m s\n",
      "BATCH \u001b[38;5;216m48\u001b[0m  Train loss: \u001b[38;5;216m0.186\u001b[0m  Time: \u001b[38;5;216m3755.0\u001b[0m s\n",
      "BATCH \u001b[38;5;216m49\u001b[0m  Train loss: \u001b[38;5;216m0.179\u001b[0m  Time: \u001b[38;5;216m3770.8\u001b[0m s\n",
      "BATCH \u001b[38;5;216m50\u001b[0m  Train loss: \u001b[38;5;216m0.185\u001b[0m  Time: \u001b[38;5;216m3786.8\u001b[0m s\n",
      "BATCH \u001b[38;5;216m51\u001b[0m  Train loss: \u001b[38;5;216m0.198\u001b[0m  Time: \u001b[38;5;216m3803.0\u001b[0m s\n",
      "BATCH \u001b[38;5;216m52\u001b[0m  Train loss: \u001b[38;5;216m0.186\u001b[0m  Time: \u001b[38;5;216m3819.7\u001b[0m s\n",
      "BATCH \u001b[38;5;216m53\u001b[0m  Train loss: \u001b[38;5;216m0.19\u001b[0m  Time: \u001b[38;5;216m3835.3\u001b[0m s\n",
      "BATCH \u001b[38;5;216m54\u001b[0m  Train loss: \u001b[38;5;216m0.172\u001b[0m  Time: \u001b[38;5;216m3850.7\u001b[0m s\n",
      "BATCH \u001b[38;5;216m55\u001b[0m  Train loss: \u001b[38;5;216m0.194\u001b[0m  Time: \u001b[38;5;216m3866.7\u001b[0m s\n",
      "BATCH \u001b[38;5;216m56\u001b[0m  Train loss: \u001b[38;5;216m0.168\u001b[0m  Time: \u001b[38;5;216m3882.4\u001b[0m s\n",
      "BATCH \u001b[38;5;216m57\u001b[0m  Train loss: \u001b[38;5;216m0.18\u001b[0m  Time: \u001b[38;5;216m3897.7\u001b[0m s\n",
      "BATCH \u001b[38;5;216m58\u001b[0m  Train loss: \u001b[38;5;216m0.174\u001b[0m  Time: \u001b[38;5;216m3913.5\u001b[0m s\n",
      "BATCH \u001b[38;5;216m59\u001b[0m  Train loss: \u001b[38;5;216m0.164\u001b[0m  Time: \u001b[38;5;216m3929.5\u001b[0m s\n",
      "BATCH \u001b[38;5;216m60\u001b[0m  Train loss: \u001b[38;5;216m0.176\u001b[0m  Time: \u001b[38;5;216m3944.8\u001b[0m s\n",
      "BATCH \u001b[38;5;216m61\u001b[0m  Train loss: \u001b[38;5;216m0.18\u001b[0m  Time: \u001b[38;5;216m3960.5\u001b[0m s\n",
      "BATCH \u001b[38;5;216m62\u001b[0m  Train loss: \u001b[38;5;216m0.175\u001b[0m  Time: \u001b[38;5;216m3975.5\u001b[0m s\n",
      "BATCH \u001b[38;5;216m63\u001b[0m  Train loss: \u001b[38;5;216m0.175\u001b[0m  Time: \u001b[38;5;216m3991.8\u001b[0m s\n",
      "BATCH \u001b[38;5;216m64\u001b[0m  Train loss: \u001b[38;5;216m0.177\u001b[0m  Time: \u001b[38;5;216m4007.4\u001b[0m s\n",
      "BATCH \u001b[38;5;216m65\u001b[0m  Train loss: \u001b[38;5;216m0.186\u001b[0m  Time: \u001b[38;5;216m4022.5\u001b[0m s\n",
      "BATCH \u001b[38;5;216m66\u001b[0m  Train loss: \u001b[38;5;216m0.181\u001b[0m  Time: \u001b[38;5;216m4037.8\u001b[0m s\n",
      "BATCH \u001b[38;5;216m67\u001b[0m  Train loss: \u001b[38;5;216m0.175\u001b[0m  Time: \u001b[38;5;216m4053.8\u001b[0m s\n",
      "BATCH \u001b[38;5;216m68\u001b[0m  Train loss: \u001b[38;5;216m0.143\u001b[0m  Time: \u001b[38;5;216m4069.3\u001b[0m s\n",
      "BATCH \u001b[38;5;216m69\u001b[0m  Train loss: \u001b[38;5;216m0.175\u001b[0m  Time: \u001b[38;5;216m4084.3\u001b[0m s\n",
      "BATCH \u001b[38;5;216m70\u001b[0m  Train loss: \u001b[38;5;216m0.176\u001b[0m  Time: \u001b[38;5;216m4099.4\u001b[0m s\n",
      "BATCH \u001b[38;5;216m71\u001b[0m  Train loss: \u001b[38;5;216m0.205\u001b[0m  Time: \u001b[38;5;216m4115.8\u001b[0m s\n",
      "BATCH \u001b[38;5;216m72\u001b[0m  Train loss: \u001b[38;5;216m0.182\u001b[0m  Time: \u001b[38;5;216m4131.0\u001b[0m s\n",
      "BATCH \u001b[38;5;216m73\u001b[0m  Train loss: \u001b[38;5;216m0.168\u001b[0m  Time: \u001b[38;5;216m4146.3\u001b[0m s\n",
      "BATCH \u001b[38;5;216m74\u001b[0m  Train loss: \u001b[38;5;216m0.193\u001b[0m  Time: \u001b[38;5;216m4161.7\u001b[0m s\n",
      "BATCH \u001b[38;5;216m75\u001b[0m  Train loss: \u001b[38;5;216m0.179\u001b[0m  Time: \u001b[38;5;216m4177.8\u001b[0m s\n",
      "BATCH \u001b[38;5;216m76\u001b[0m  Train loss: \u001b[38;5;216m0.178\u001b[0m  Time: \u001b[38;5;216m4193.1\u001b[0m s\n",
      "BATCH \u001b[38;5;216m77\u001b[0m  Train loss: \u001b[38;5;216m0.182\u001b[0m  Time: \u001b[38;5;216m4208.4\u001b[0m s\n",
      "BATCH \u001b[38;5;216m78\u001b[0m  Train loss: \u001b[38;5;216m0.175\u001b[0m  Time: \u001b[38;5;216m4224.2\u001b[0m s\n",
      "BATCH \u001b[38;5;216m79\u001b[0m  Train loss: \u001b[38;5;216m0.152\u001b[0m  Time: \u001b[38;5;216m4240.8\u001b[0m s\n",
      "BATCH \u001b[38;5;216m80\u001b[0m  Train loss: \u001b[38;5;216m0.197\u001b[0m  Time: \u001b[38;5;216m4256.7\u001b[0m s\n",
      "BATCH \u001b[38;5;216m81\u001b[0m  Train loss: \u001b[38;5;216m0.163\u001b[0m  Time: \u001b[38;5;216m4272.3\u001b[0m s\n",
      "BATCH \u001b[38;5;216m82\u001b[0m  Train loss: \u001b[38;5;216m0.177\u001b[0m  Time: \u001b[38;5;216m4288.5\u001b[0m s\n",
      "BATCH \u001b[38;5;216m83\u001b[0m  Train loss: \u001b[38;5;216m0.194\u001b[0m  Time: \u001b[38;5;216m4303.8\u001b[0m s\n",
      "BATCH \u001b[38;5;216m84\u001b[0m  Train loss: \u001b[38;5;216m0.178\u001b[0m  Time: \u001b[38;5;216m4319.3\u001b[0m s\n",
      "BATCH \u001b[38;5;216m85\u001b[0m  Train loss: \u001b[38;5;216m0.199\u001b[0m  Time: \u001b[38;5;216m4334.5\u001b[0m s\n",
      "BATCH \u001b[38;5;216m86\u001b[0m  Train loss: \u001b[38;5;216m0.196\u001b[0m  Time: \u001b[38;5;216m4350.7\u001b[0m s\n",
      "BATCH \u001b[38;5;216m87\u001b[0m  Train loss: \u001b[38;5;216m0.17\u001b[0m  Time: \u001b[38;5;216m4366.2\u001b[0m s\n",
      "BATCH \u001b[38;5;216m88\u001b[0m  Train loss: \u001b[38;5;216m0.173\u001b[0m  Time: \u001b[38;5;216m4381.8\u001b[0m s\n",
      "BATCH \u001b[38;5;216m89\u001b[0m  Train loss: \u001b[38;5;216m0.158\u001b[0m  Time: \u001b[38;5;216m4397.2\u001b[0m s\n",
      "BATCH \u001b[38;5;216m90\u001b[0m  Train loss: \u001b[38;5;216m0.163\u001b[0m  Time: \u001b[38;5;216m4413.0\u001b[0m s\n",
      "BATCH \u001b[38;5;216m91\u001b[0m  Train loss: \u001b[38;5;216m0.175\u001b[0m  Time: \u001b[38;5;216m4428.5\u001b[0m s\n",
      "BATCH \u001b[38;5;216m92\u001b[0m  Train loss: \u001b[38;5;216m0.168\u001b[0m  Time: \u001b[38;5;216m4443.8\u001b[0m s\n",
      "BATCH \u001b[38;5;216m93\u001b[0m  Train loss: \u001b[38;5;216m0.157\u001b[0m  Time: \u001b[38;5;216m4459.2\u001b[0m s\n",
      "BATCH \u001b[38;5;216m94\u001b[0m  Train loss: \u001b[38;5;216m0.195\u001b[0m  Time: \u001b[38;5;216m4475.7\u001b[0m s\n",
      "BATCH \u001b[38;5;216m95\u001b[0m  Train loss: \u001b[38;5;216m0.187\u001b[0m  Time: \u001b[38;5;216m4494.1\u001b[0m s\n",
      "BATCH \u001b[38;5;216m96\u001b[0m  Train loss: \u001b[38;5;216m0.19\u001b[0m  Time: \u001b[38;5;216m4510.0\u001b[0m s\n",
      "BATCH \u001b[38;5;216m97\u001b[0m  Train loss: \u001b[38;5;216m0.189\u001b[0m  Time: \u001b[38;5;216m4526.3\u001b[0m s\n",
      "BATCH \u001b[38;5;216m98\u001b[0m  Train loss: \u001b[38;5;216m0.2\u001b[0m  Time: \u001b[38;5;216m4542.3\u001b[0m s\n",
      "BATCH \u001b[38;5;216m99\u001b[0m  Train loss: \u001b[38;5;216m0.159\u001b[0m  Time: \u001b[38;5;216m4557.9\u001b[0m s\n",
      "BATCH \u001b[38;5;216m100\u001b[0m  Train loss: \u001b[38;5;216m0.192\u001b[0m  Time: \u001b[38;5;216m4573.2\u001b[0m s\n",
      "BATCH \u001b[38;5;216m101\u001b[0m  Train loss: \u001b[38;5;216m0.152\u001b[0m  Time: \u001b[38;5;216m4589.0\u001b[0m s\n",
      "BATCH \u001b[38;5;216m102\u001b[0m  Train loss: \u001b[38;5;216m0.187\u001b[0m  Time: \u001b[38;5;216m4604.6\u001b[0m s\n",
      "BATCH \u001b[38;5;216m103\u001b[0m  Train loss: \u001b[38;5;216m0.188\u001b[0m  Time: \u001b[38;5;216m4619.9\u001b[0m s\n",
      "BATCH \u001b[38;5;216m104\u001b[0m  Train loss: \u001b[38;5;216m0.158\u001b[0m  Time: \u001b[38;5;216m4635.2\u001b[0m s\n",
      "BATCH \u001b[38;5;216m105\u001b[0m  Train loss: \u001b[38;5;216m0.202\u001b[0m  Time: \u001b[38;5;216m4651.3\u001b[0m s\n",
      "BATCH \u001b[38;5;216m106\u001b[0m  Train loss: \u001b[38;5;216m0.172\u001b[0m  Time: \u001b[38;5;216m4666.5\u001b[0m s\n",
      "BATCH \u001b[38;5;216m107\u001b[0m  Train loss: \u001b[38;5;216m0.173\u001b[0m  Time: \u001b[38;5;216m4681.6\u001b[0m s\n",
      "BATCH \u001b[38;5;216m108\u001b[0m  Train loss: \u001b[38;5;216m0.184\u001b[0m  Time: \u001b[38;5;216m4696.9\u001b[0m s\n",
      "BATCH \u001b[38;5;216m109\u001b[0m  Train loss: \u001b[38;5;216m0.186\u001b[0m  Time: \u001b[38;5;216m4713.1\u001b[0m s\n",
      "BATCH \u001b[38;5;216m110\u001b[0m  Train loss: \u001b[38;5;216m0.198\u001b[0m  Time: \u001b[38;5;216m4728.7\u001b[0m s\n",
      "BATCH \u001b[38;5;216m111\u001b[0m  Train loss: \u001b[38;5;216m0.18\u001b[0m  Time: \u001b[38;5;216m4744.2\u001b[0m s\n",
      "BATCH \u001b[38;5;216m112\u001b[0m  Train loss: \u001b[38;5;216m0.193\u001b[0m  Time: \u001b[38;5;216m4759.4\u001b[0m s\n",
      "BATCH \u001b[38;5;216m113\u001b[0m  Train loss: \u001b[38;5;216m0.183\u001b[0m  Time: \u001b[38;5;216m4775.6\u001b[0m s\n",
      "BATCH \u001b[38;5;216m114\u001b[0m  Train loss: \u001b[38;5;216m0.174\u001b[0m  Time: \u001b[38;5;216m4791.2\u001b[0m s\n",
      "BATCH \u001b[38;5;216m115\u001b[0m  Train loss: \u001b[38;5;216m0.173\u001b[0m  Time: \u001b[38;5;216m4806.4\u001b[0m s\n",
      "BATCH \u001b[38;5;216m116\u001b[0m  Train loss: \u001b[38;5;216m0.17\u001b[0m  Time: \u001b[38;5;216m4821.6\u001b[0m s\n",
      "BATCH \u001b[38;5;216m117\u001b[0m  Train loss: \u001b[38;5;216m0.18\u001b[0m  Time: \u001b[38;5;216m4837.8\u001b[0m s\n",
      "BATCH \u001b[38;5;216m118\u001b[0m  Train loss: \u001b[38;5;216m0.163\u001b[0m  Time: \u001b[38;5;216m4852.8\u001b[0m s\n",
      "BATCH \u001b[38;5;216m119\u001b[0m  Train loss: \u001b[38;5;216m0.2\u001b[0m  Time: \u001b[38;5;216m4868.2\u001b[0m s\n",
      "BATCH \u001b[38;5;216m120\u001b[0m  Train loss: \u001b[38;5;216m0.187\u001b[0m  Time: \u001b[38;5;216m4883.6\u001b[0m s\n",
      "BATCH \u001b[38;5;216m121\u001b[0m  Train loss: \u001b[38;5;216m0.164\u001b[0m  Time: \u001b[38;5;216m4899.5\u001b[0m s\n",
      "BATCH \u001b[38;5;216m122\u001b[0m  Train loss: \u001b[38;5;216m0.189\u001b[0m  Time: \u001b[38;5;216m4915.4\u001b[0m s\n",
      "BATCH \u001b[38;5;216m123\u001b[0m  Train loss: \u001b[38;5;216m0.186\u001b[0m  Time: \u001b[38;5;216m4930.3\u001b[0m s\n",
      "BATCH \u001b[38;5;216m124\u001b[0m  Train loss: \u001b[38;5;216m0.191\u001b[0m  Time: \u001b[38;5;216m4945.7\u001b[0m s\n",
      "BATCH \u001b[38;5;216m125\u001b[0m  Train loss: \u001b[38;5;216m0.182\u001b[0m  Time: \u001b[38;5;216m4962.0\u001b[0m s\n",
      "BATCH \u001b[38;5;216m126\u001b[0m  Train loss: \u001b[38;5;216m0.174\u001b[0m  Time: \u001b[38;5;216m4977.0\u001b[0m s\n",
      "BATCH \u001b[38;5;216m127\u001b[0m  Train loss: \u001b[38;5;216m0.163\u001b[0m  Time: \u001b[38;5;216m4992.2\u001b[0m s\n",
      "BATCH \u001b[38;5;216m128\u001b[0m  Train loss: \u001b[38;5;216m0.169\u001b[0m  Time: \u001b[38;5;216m5008.3\u001b[0m s\n",
      "BATCH \u001b[38;5;216m129\u001b[0m  Train loss: \u001b[38;5;216m0.175\u001b[0m  Time: \u001b[38;5;216m5023.6\u001b[0m s\n",
      "BATCH \u001b[38;5;216m130\u001b[0m  Train loss: \u001b[38;5;216m0.195\u001b[0m  Time: \u001b[38;5;216m5038.9\u001b[0m s\n",
      "BATCH \u001b[38;5;216m131\u001b[0m  Train loss: \u001b[38;5;216m0.203\u001b[0m  Time: \u001b[38;5;216m5054.4\u001b[0m s\n",
      "BATCH \u001b[38;5;216m132\u001b[0m  Train loss: \u001b[38;5;216m0.183\u001b[0m  Time: \u001b[38;5;216m5070.4\u001b[0m s\n",
      "BATCH \u001b[38;5;216m133\u001b[0m  Train loss: \u001b[38;5;216m0.16\u001b[0m  Time: \u001b[38;5;216m5085.6\u001b[0m s\n",
      "BATCH \u001b[38;5;216m134\u001b[0m  Train loss: \u001b[38;5;216m0.163\u001b[0m  Time: \u001b[38;5;216m5100.5\u001b[0m s\n",
      "BATCH \u001b[38;5;216m135\u001b[0m  Train loss: \u001b[38;5;216m0.149\u001b[0m  Time: \u001b[38;5;216m5115.9\u001b[0m s\n",
      "BATCH \u001b[38;5;216m136\u001b[0m  Train loss: \u001b[38;5;216m0.186\u001b[0m  Time: \u001b[38;5;216m5132.2\u001b[0m s\n",
      "BATCH \u001b[38;5;216m137\u001b[0m  Train loss: \u001b[38;5;216m0.187\u001b[0m  Time: \u001b[38;5;216m5147.6\u001b[0m s\n",
      "BATCH \u001b[38;5;216m138\u001b[0m  Train loss: \u001b[38;5;216m0.155\u001b[0m  Time: \u001b[38;5;216m5162.5\u001b[0m s\n",
      "BATCH \u001b[38;5;216m139\u001b[0m  Train loss: \u001b[38;5;216m0.167\u001b[0m  Time: \u001b[38;5;216m5177.7\u001b[0m s\n",
      "BATCH \u001b[38;5;216m140\u001b[0m  Train loss: \u001b[38;5;216m0.21\u001b[0m  Time: \u001b[38;5;216m5193.9\u001b[0m s\n",
      "BATCH \u001b[38;5;216m141\u001b[0m  Train loss: \u001b[38;5;216m0.174\u001b[0m  Time: \u001b[38;5;216m5209.2\u001b[0m s\n",
      "BATCH \u001b[38;5;216m142\u001b[0m  Train loss: \u001b[38;5;216m0.173\u001b[0m  Time: \u001b[38;5;216m5224.3\u001b[0m s\n",
      "BATCH \u001b[38;5;216m143\u001b[0m  Train loss: \u001b[38;5;216m0.19\u001b[0m  Time: \u001b[38;5;216m5239.7\u001b[0m s\n",
      "BATCH \u001b[38;5;216m144\u001b[0m  Train loss: \u001b[38;5;216m0.193\u001b[0m  Time: \u001b[38;5;216m5255.7\u001b[0m s\n",
      "BATCH \u001b[38;5;216m145\u001b[0m  Train loss: \u001b[38;5;216m0.199\u001b[0m  Time: \u001b[38;5;216m5270.9\u001b[0m s\n",
      "BATCH \u001b[38;5;216m146\u001b[0m  Train loss: \u001b[38;5;216m0.154\u001b[0m  Time: \u001b[38;5;216m5286.1\u001b[0m s\n",
      "BATCH \u001b[38;5;216m147\u001b[0m  Train loss: \u001b[38;5;216m0.2\u001b[0m  Time: \u001b[38;5;216m5300.2\u001b[0m s\n",
      "\n",
      "\n",
      "EPOCH \u001b[38;5;216m2\u001b[0m  Val loss: \u001b[38;5;216m0.179\u001b[0m  Time: \u001b[38;5;216m5859.0\u001b[0m s\n",
      "\n",
      "\n",
      "ENDING TRAINING ...\n"
     ]
    }
   ],
   "source": [
    "print(\"STARTING TRAINING ...\\n\")\n",
    "\n",
    "train_preds_final, test_preds_final = train_resnet18()\n",
    "    \n",
    "print(\"ENDING TRAINING ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(train_preds_final, columns=targets.columns[1:], index=train_df['Id']).to_csv('mean_image_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(test_preds_final, columns=targets.columns[1:], index=test_df['Id']).to_csv('mean_image_test.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "1607bb5bfa7e4e4d888ef665a6c1cc4b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3a3c138767ee4c1d8e9d5ad7c1aef6d1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_8e7e3c181f344d3bbbf0b6549b814e1a",
       "max": 46827520.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_b2afa906182d4cd8afbd3d9e83ebea23",
       "value": 46827520.0
      }
     },
     "4e51216d60064d0c8a57b18fb7bc770a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_c3a13c19c3ce4a67ae356c40440df088",
       "placeholder": "​",
       "style": "IPY_MODEL_f265e8ea165d4223900f69c4e871d57e",
       "value": " 44.7M/44.7M [00:05&lt;00:00, 8.03MB/s]"
      }
     },
     "738088042b2b4656a1481f5971c0e9c2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_3a3c138767ee4c1d8e9d5ad7c1aef6d1",
        "IPY_MODEL_4e51216d60064d0c8a57b18fb7bc770a"
       ],
       "layout": "IPY_MODEL_1607bb5bfa7e4e4d888ef665a6c1cc4b"
      }
     },
     "8e7e3c181f344d3bbbf0b6549b814e1a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b2afa906182d4cd8afbd3d9e83ebea23": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "c3a13c19c3ce4a67ae356c40440df088": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f265e8ea165d4223900f69c4e871d57e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
