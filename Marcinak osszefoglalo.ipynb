{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# features from ic and fnc usig modified rapids svm notebook\n",
    "#####\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "# import pandas as pd\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# wheited error for loss\n",
    "def metric(y_true, y_pred):\n",
    "    return np.mean(np.sum(np.abs(y_true - y_pred), axis=0)/np.sum(y_true, axis=0))\n",
    "\n",
    "\n",
    "fnc_df = pd.read_csv(\"/kaggle/input/trends-assessment-prediction/fnc.csv\")\n",
    "loading_df = pd.read_csv(\"/kaggle/input/trends-assessment-prediction/loading.csv\")\n",
    "\n",
    "\n",
    "fnc_features, loading_features = list(fnc_df.columns[1:]), list(loading_df.columns[1:])\n",
    "df = fnc_df.merge(loading_df, on=\"Id\")\n",
    "\n",
    "\n",
    "labels_df = pd.read_csv(\"/kaggle/input/trends-assessment-prediction/train_scores.csv\")\n",
    "labels_df[\"is_train\"] = True\n",
    "\n",
    "df = df.merge(labels_df, on=\"Id\", how=\"left\")\n",
    "\n",
    "test_df = df[df[\"is_train\"] != True].copy()\n",
    "df = df[df[\"is_train\"] == True].copy()\n",
    "\n",
    "df.shape, test_df.shape\n",
    "\n",
    "# Giving less importance to FNC features since they are easier to overfit due to high dimensionality.\n",
    "FNC_SCALE = 1/500\n",
    "\n",
    "df[fnc_features] *= FNC_SCALE\n",
    "test_df[fnc_features] *= FNC_SCALE\n",
    "\n",
    "%%time\n",
    "\n",
    "NUM_FOLDS = 7\n",
    "kf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=0)\n",
    "\n",
    "target_pred = {'age':'age_pred', 'domain1_var1':'domain1_var1_pred',\n",
    "               'domain1_var2':'domain1_var2_pred','domain2_var1': 'domain2_var1_pred',\n",
    "               'domain2_var2': 'domain2_var2_pred'}\n",
    "\n",
    "features = loading_features + fnc_features\n",
    "\n",
    "overal_score = 0\n",
    "# for feature, C(smoothing factor) w(weight) for scoringd\n",
    "for target, c, w in [(\"age\", 100, 0.3), (\"domain1_var1\", 10, 0.175), (\"domain1_var2\", 10, 0.175), (\"domain2_var1\", 10, 0.175), (\"domain2_var2\", 10, 0.175)]:    \n",
    "    y_oof = np.zeros(df.shape[0])\n",
    "    y_test = np.zeros((test_df.shape[0], NUM_FOLDS))\n",
    "    y_train = np.zeros((train_df.shape[0], NUM_FOLDS))\n",
    "    \n",
    "    for f, (train_ind, val_ind) in enumerate(kf.split(df, df)):\n",
    "        train_df, val_df = df.iloc[train_ind], df.iloc[val_ind]\n",
    "        train_df = train_df[train_df[target].notnull()]\n",
    "        \n",
    "        # fitting the model\n",
    "        model = SVR(C=c, cache_size=3000.0, verbose=True)\n",
    "        model.fit(train_df[features], train_df[target])\n",
    "        \n",
    "        # train set predictions\n",
    "        y_oof[val_ind] = model.predict(val_df[features])\n",
    "        # test set predictions\n",
    "        y_test[:, f] = model.predict(test_df[features])\n",
    "        \n",
    "    df[\"pred_{}\".format(target)] = y_oof\n",
    "    # mean of models\n",
    "    test_df[target] = y_test.mean(axis=1)\n",
    "    score = metric(df[df[target].notnull()][target].values, df[df[target].notnull()][\"pred_{}\".format(target)].values)\n",
    "    overal_score += w*score\n",
    "    print(target, np.round(score, 4))\n",
    "    print()\n",
    "    \n",
    "print(\"Overal score:\", np.round(overal_score, 4))\n",
    "\n",
    "\n",
    "# making submission\n",
    "sub_df = pd.melt(test_df[[\"Id\", \"age\", \"domain1_var1\", \"domain1_var2\", \"domain2_var1\", \"domain2_var2\"]], id_vars=[\"Id\"], value_name=\"Predicted\")\n",
    "sub_df[\"Id\"] = sub_df[\"Id\"].astype(\"str\") + \"_\" +  sub_df[\"variable\"].astype(\"str\")\n",
    "\n",
    "sub_df = sub_df.drop(\"variable\", axis=1).sort_values(\"Id\")\n",
    "assert sub_df.shape[0] == test_df.shape[0]*5\n",
    "sub_df.head(10)\n",
    "\n",
    "sub_df.to_csv(\"fnc_ic_test_sub.csv\", index=False)\n",
    "\n",
    "# making sets for next level svm ensemble\n",
    "test_df[[\"Id\", \"age\", \"domain1_var1\", \"domain1_var2\", \"domain2_var1\", \"domain2_var2\"]].to_csv('fnc_ic_test_pred')\n",
    "\n",
    "df[[\"Id\", \"pred_age\", \"pred_domain1_var1\", \"pred_domain1_var2\", \"pred_domain2_var1\", \"pred_domain2_var2\"]].to_csv('fnc_ic_train_pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all nesseccary and shitty libraries\n",
    "!pip install -q colored\n",
    "!pip install -q torchviz\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import cv2\n",
    "import time\n",
    "import h5py\n",
    "import colored\n",
    "from colored import fg, bg, attr\n",
    "\n",
    "from skimage import measure\n",
    "from plotly.offline import iplot\n",
    "from plotly import figure_factory as FF\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import randint\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torchviz import make_dot\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam, RMSprop, SparseAdam, Adamax\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torchvision.models import resnet18, densenet121, mobilenet_v2\n",
    "\n",
    "## Set hyperparameters and paths <a id=\"1.2\"></a> <font color=\"#fa7703\" size=4>(adjust these to improve LB scores :D)</font>\n",
    "\n",
    "1. Choose hyperparameters such as epochs, split percentage, learning rate, etc\n",
    "2. Set important paths and directories to load data and train the model\n",
    "\n",
    "EPOCHS = 5\n",
    "SPLIT = 0.8\n",
    "LR = (1e-4, 1e-3)\n",
    "MODEL_SAVE_PATH = \"resnet_model\"\n",
    "\n",
    "W = 192\n",
    "H = 64\n",
    "loading_scale = 500\n",
    "BATCH_SIZE = 32\n",
    "VAL_BATCH_SIZE = 32\n",
    "DATA_PATH = '../input/trends-assessment-prediction/'\n",
    "\n",
    "## Load .csv data <a id=\"1.3\"></a> <font color=\"#fa7703\" size=4>(to access 3D fMRI maps for training and validation)</font>\n",
    "\n",
    "1. Load dataframes with IDs, targets, and tabular features\n",
    "2. Extract relevant IDs for training and testing from the dataframes\n",
    "\n",
    "TEST_MAP_PATH = DATA_PATH + 'fMRI_test/'\n",
    "TRAIN_MAP_PATH = DATA_PATH + 'fMRI_train/'\n",
    "\n",
    "FEAT_PATH = DATA_PATH + 'fnc.csv'\n",
    "TARG_PATH = DATA_PATH + 'train_scores.csv'\n",
    "SAMPLE_SUB_PATH = DATA_PATH + 'sample_submission.csv'\n",
    "\n",
    "TEST_IDS = [map_id[:-4] for map_id in sorted(os.listdir(TEST_MAP_PATH))]\n",
    "TRAIN_IDS = [map_id[:-4] for map_id in sorted(os.listdir(TRAIN_MAP_PATH))]\n",
    "\n",
    "targets = pd.read_csv(TARG_PATH)\n",
    "targets = targets.fillna(targets.mean())\n",
    "sample_submission = pd.read_csv(SAMPLE_SUB_PATH)\n",
    "\n",
    "features = pd.read_csv(FEAT_PATH)\n",
    "test_df = features.query('Id in {}'.format(TEST_IDS)).reset_index(drop=True)\n",
    "train_df = features.query('Id in {}'.format(TRAIN_IDS)).reset_index(drop=True)\n",
    "\n",
    "# Modeling <a id=\"2\"></a>\n",
    "\n",
    "## Build PyTorch Dataset <a id=\"2.1\"></a> <font color=\"#fa7703\" size=4>(with map slicing and resizing)</font>\n",
    "\n",
    "1. Retrieve all 53 fMRI maps for a given ID\n",
    "2. Randomly slice each map along the *x, y,* and *z* axes to get 159 2D slices\n",
    "3. Resize each 2D map to the shape (64, 64) using OpenCV-2 and concatenate all slices\n",
    "4. Get targets for given ID and stack 159 times (each slice has the same target)\n",
    "5. Return the calculated image tensors and target tensors for the given patient ID\n",
    "\n",
    "loading = pd.read_csv('/kaggle/input/trends-assessment-prediction/loading.csv')\n",
    "loading = loading.set_index('Id')\n",
    "\n",
    "\n",
    "\n",
    "# code same as pytorch data...... but took too long in this script\n",
    "# i left the code for reference\n",
    "\n",
    "# get paths of preproccessed files\n",
    "# these slice files were mde using nilean, find xyz_cut_coords\n",
    "import os\n",
    "import fnmatch\n",
    "\n",
    "BASEPATH = '/kaggle/input/'\n",
    "\n",
    "dir_list = ['pytorch-data-train-1', 'pytorch-data-train-2', 'pytorch-data-train-3', 'pytorch-data-train-4', 'pytorch-data-train-5', 'pytorch-data-train-6', 'pytorch-data-train-7',\n",
    "           'pytorch-data-test-1', 'pytorch-data-test-2', 'pytorch-data-test-3', 'pytorch-data-test-4', 'pytorch-data-test-5', 'pytorch-data-test-6', 'pytorch-data-test-7',\n",
    "           'missing-files-1', 'missing-files-2', 'missing-files-3', 'missing-files-4']\n",
    "files = {}\n",
    "for directory in dir_list:\n",
    "    for file in os.listdir(BASEPATH + directory + '/'):\n",
    "        if fnmatch.fnmatch(file, '*.npy'):\n",
    "            files[file[:-4]] = BASEPATH + directory + '/' + file\n",
    "\n",
    "class TReNDSDataset(Dataset):\n",
    "    # initialize the dataset\n",
    "    def __init__(self, data, targets, map_path, is_train):\n",
    "        self.data = data\n",
    "        self.is_train = is_train\n",
    "        self.map_path = map_path\n",
    "        self.map_id = self.data.Id\n",
    "        if is_train: self.targets = targets\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load preprosseced files\n",
    "        path_npy = files[str(self.map_id[idx])]\n",
    "        npy = np.load(path_npy)\n",
    "        # concatinate only used if you want to mess around with the size of the slices\n",
    "        features = np.concatenate([npy[::3, :, :], npy[1::3, :, :], npy[2::3, :, :]], axis=1)\n",
    "        \n",
    "        if not self.is_train:\n",
    "            # if predictiong return without targets\n",
    "            return torch.FloatTensor(features)\n",
    "        else:\n",
    "            i = self.map_id[idx]\n",
    "            targets = self.targets.query('Id == {}'.format(i)).values\n",
    "            targets = np.repeat(targets[:, 1:], 53, 0).reshape(-1, 5)\n",
    "            return torch.FloatTensor(features), torch.FloatTensor(targets)\n",
    "\n",
    "# npy = np.load(files['14696'])\n",
    "# np.concatenate([npy[::3, :, :], npy[1::3, :, :], npy[2::3, :, :]], axis=1).shape\n",
    "\n",
    "## Build ResNet model <a id=\"2.2\"></a> <font color=\"#fa7703\" size=4>(with a double dense head)</font>\n",
    "\n",
    "1. Get ResNet-18 head (till AvgPool)\n",
    "2. Add two Dense layers (16 and 5 neurons) on top\n",
    "3. Reshape and tile image to match ResNet input dimensions\n",
    "4. Pass reshaped image tensor through neural network and get output\n",
    "\n",
    "\n",
    "class ResNetModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNetModel, self).__init__()\n",
    "        \n",
    "        self.identity = lambda x: x\n",
    "        self.dense_out = nn.Linear(16, 5)\n",
    "        self.dense_in = nn.Linear(512, 16)\n",
    "        # load pretrained model from pytorch deep visionimagenet lib\n",
    "        self.resnet = resnet18(pretrained=True, progress=False)\n",
    "        self.resnet = nn.Sequential(*list(self.resnet.children())[:-1])\n",
    "        \n",
    "    def forward(self, img):\n",
    "        # reshape image for compatibility with model\n",
    "        img = img.reshape(-1, 1, H, W)\n",
    "        feat = self.resnet(img.repeat(1, 3, 1, 1))\n",
    "        \n",
    "        conc = self.dense_in(feat.squeeze())\n",
    "        return self.identity(self.dense_out(conc))\n",
    "\n",
    "## Define custom weighted absolute error loss<a id=\"2.4\"></a> <font color=\"#fa7703\" size=4>(for backpropagation)</font>\n",
    "\n",
    "1. Define weightage for each target\n",
    "2. Calculate weighted absolute errors and take the average\n",
    "\n",
    "def weighted_nae(inp, targ):\n",
    "    W = torch.FloatTensor([0.3, 0.175, 0.175, 0.175, 0.175])\n",
    "    return torch.mean(torch.matmul(torch.abs(inp - targ), W.cuda()/torch.mean(targ, axis=0)))\n",
    "\n",
    "## Define helper function for training logs <a id=\"2.5\"></a> <font color=\"#fa7703\" size=4>(to check training status)</font>\n",
    "\n",
    "1. Retrieve training and validation metrics\n",
    "2. Format metrics and display them during training\n",
    "\n",
    "def print_metric(data, batch, epoch, start, end, metric, typ):\n",
    "    time = np.round(end - start, 1)\n",
    "    time = \"Time: %s{}%s s\".format(time)\n",
    "\n",
    "    if typ == \"Train\":\n",
    "        pre = \"BATCH %s\" + str(batch-1) + \"%s  \"\n",
    "    if typ == \"Val\":\n",
    "        pre = \"EPOCH %s\" + str(epoch+1) + \"%s  \"\n",
    "    \n",
    "    fonts = (fg(216), attr('reset'))\n",
    "    value = np.round(data.item(), 3)\n",
    "    t = typ, metric, \"%s\", value, \"%s\"\n",
    "\n",
    "    print(pre % fonts , end='')\n",
    "    print(\"{} {}: {}{}{}\".format(*t) % fonts + \"  \" + time % fonts)\n",
    "\n",
    "## Split data into training and validation sets <a id=\"2.6\"></a> <font color=\"#fa7703\" size=4>(to validate performance properly)</font>\n",
    "\n",
    "1. Split the data into training and validation sets\n",
    "2. Define the test data loader to run inference after training\n",
    "\n",
    "val_out_shape = -1, 5\n",
    "train_out_shape = -1, 5\n",
    "\n",
    "split = int(SPLIT*len(train_df))\n",
    "val = train_df[split:].reset_index(drop=True)\n",
    "train = train_df[:split].reset_index(drop=True)\n",
    "\n",
    "test_set_p = TReNDSDataset(train_df, None, TRAIN_MAP_PATH, False)\n",
    "test_loader_p = DataLoader(test_set_p, batch_size=VAL_BATCH_SIZE)\n",
    "\n",
    "test_set = TReNDSDataset(test_df, None, TEST_MAP_PATH, False)\n",
    "test_loader = DataLoader(test_set, batch_size=VAL_BATCH_SIZE)\n",
    "\n",
    "## Train model on GPU <a id=\"2.7\"></a> <font color=\"#fa7703\" size=4>(NVIDIA Tesla P100)</font>\n",
    "\n",
    "1. Define dataloders, model, optimizer, and learning rate scheduler\n",
    "2. Train the model in batches and check validation performance at the end of each epoch\n",
    "3. Save the model architecture and weights and generate testing predictions after training\n",
    "\n",
    "def train_resnet18():\n",
    "    def cuda(tensor):\n",
    "        return tensor.cuda()\n",
    "   \n",
    "    val_set = TReNDSDataset(val, targets, TRAIN_MAP_PATH, True)\n",
    "    val_loader = DataLoader(val_set, batch_size=VAL_BATCH_SIZE)\n",
    "    train_set = TReNDSDataset(train, targets, TRAIN_MAP_PATH, True)\n",
    "    train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    network = cuda(ResNetModel())\n",
    "    optimizer =  Adam([{'params': network.resnet.parameters(), 'lr': LR[0]},\n",
    "                      {'params': network.dense_in.parameters(), 'lr': LR[1]},\n",
    "                      {'params': network.dense_out.parameters(), 'lr': LR[1]}])\n",
    "\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.95,\n",
    "                                  patience=4, verbose=True, eps=1e-6)\n",
    "    start = time.time()\n",
    "    for epoch in range(EPOCHS):\n",
    "        batch = 1\n",
    "        fonts = (fg(216), attr('reset'))\n",
    "        print((\"EPOCH %s\" + str(epoch+1) + \"%s\") % fonts)\n",
    "\n",
    "        for train_batch in train_loader:\n",
    "            train_img, train_targs = train_batch\n",
    "           \n",
    "            network.train()\n",
    "            network = cuda(network)\n",
    "            train_preds = network.forward(cuda(train_img))\n",
    "            train_targs = train_targs.reshape(train_out_shape)\n",
    "            train_loss = weighted_nae(train_preds, cuda(train_targs))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            train_loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            end = time.time()\n",
    "            batch = batch + 1\n",
    "            print_metric(train_loss, batch, epoch, start, end, metric=\"loss\", typ=\"Train\")\n",
    "            \n",
    "        print(\"\\n\")\n",
    "           \n",
    "        network.eval()\n",
    "        for val_batch in val_loader:\n",
    "            img, targ = val_batch\n",
    "            val_preds, val_targs = [], []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                img = cuda(img)\n",
    "                network = cuda(network)\n",
    "                pred = network.forward(img)\n",
    "                val_preds.append(pred); val_targs.append(targ)\n",
    "\n",
    "        val_preds = torch.cat(val_preds, axis=0)\n",
    "        val_targs = torch.cat(val_targs, axis=0)\n",
    "        val_targs = val_targs.reshape(val_out_shape)\n",
    "        val_loss = weighted_nae(val_preds, cuda(val_targs))\n",
    "        \n",
    "        avg_preds = []\n",
    "        avg_targs = []\n",
    "        for idx in range(0, len(val_preds), 53):\n",
    "            avg_preds.append(val_preds[idx:idx+53].mean(axis=0))\n",
    "            avg_targs.append(val_targs[idx:idx+53].mean(axis=0))\n",
    "            \n",
    "        avg_preds = torch.stack(avg_preds, axis=0)\n",
    "        avg_targs = torch.stack(avg_targs, axis=0)\n",
    "        loss = weighted_nae(avg_preds, cuda(avg_targs))\n",
    "        \n",
    "        end = time.time()\n",
    "        scheduler.step(val_loss)\n",
    "        print_metric(loss, None, epoch, start, end, metric=\"loss\", typ=\"Val\")\n",
    "        \n",
    "        print(\"\\n\")\n",
    "        \n",
    "        # copied two times down here so model also makes predictions on the training set\n",
    "   \n",
    "    network.eval()\n",
    "    if os.path.exists(TRAIN_MAP_PATH):\n",
    "\n",
    "        test_preds = []\n",
    "        for test_img in test_loader:\n",
    "            with torch.no_grad():\n",
    "                network = cuda(network)\n",
    "                test_img = cuda(test_img)\n",
    "                test_preds.append(network.forward(test_img))\n",
    "                \n",
    "                \n",
    "                \n",
    "        test_preds_p = []\n",
    "        for test_img_p in test_loader_p:\n",
    "            with torch.no_grad():\n",
    "                network = cuda(network)\n",
    "                test_img_p = cuda(test_img_p)\n",
    "                test_preds_p.append(network.forward(test_img_p))\n",
    "        \n",
    "        \n",
    "        avg_preds = []\n",
    "        test_preds = torch.cat(test_preds, axis=0)\n",
    "        for idx in range(0, len(test_preds), 53):\n",
    "            avg_preds.append(test_preds[idx:idx+53].mean(axis=0))\n",
    "            \n",
    "        avg_preds_p = []\n",
    "        test_preds_p = torch.cat(test_preds_p, axis=0)\n",
    "        for idx in range(0, len(test_preds_p), 53):\n",
    "            avg_preds_p.append(test_preds_p[idx:idx+53].mean(axis=0))\n",
    "\n",
    "\n",
    "        return torch.stack(avg_preds_p, axis=0).detach().cpu().numpy(), torch.stack(avg_preds, axis=0).detach().cpu().numpy()\n",
    "\n",
    "print(\"STARTING TRAINING ...\\n\")\n",
    "\n",
    "# train resnet get results for second level ensemble model\n",
    "train_preds_final, test_preds_final = train_resnet18()\n",
    "    \n",
    "print(\"ENDING TRAINING ...\")\n",
    "\n",
    "pd.DataFrame(train_preds_final, columns=targets.columns[1:], index=train_df['Id']).to_csv('res_preds_traintime2.csv')\n",
    "\n",
    "pd.DataFrame(test_preds_final, columns=targets.columns[1:], index=test_df['Id']).to_csv('res_preds_testtime2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# load csv files\n",
    "loading = pd.read_csv('/kaggle/input/trends-assessment-prediction/loading.csv')\n",
    "train_scores = pd.read_csv('/kaggle/input/trends-assessment-prediction/train_scores.csv')\n",
    "\n",
    "# replace nan with mean of columns\n",
    "data_list = [loading, train_scores]\n",
    "\n",
    "for data in data_list:\n",
    "    for col in data.columns:\n",
    "        data[col].fillna(data[col].mean(), inplace=True)\n",
    "        \n",
    "loading = loading[sorted(loading.columns)]\n",
    "\n",
    "# {'age': SVR(C=50, cache_size=200, coef0=0.0, degree=3, epsilon=1.1, gamma='scale',\n",
    "#      kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False),\n",
    "#  'domain1_var1': SVR(C=1000, cache_size=200, coef0=0.0, degree=3, epsilon=0.9, gamma='scale',\n",
    "#      kernel='linear', max_iter=-1, shrinking=True, tol=0.001, verbose=False),\n",
    "#  'domain1_var2': SVR(C=5, cache_size=200, coef0=0.0, degree=3, epsilon=0.01, gamma='scale',\n",
    "#      kernel='linear', max_iter=-1, shrinking=True, tol=0.001, verbose=False),\n",
    "#  'domain2_var1': SVR(C=5, cache_size=200, coef0=0.0, degree=3, epsilon=0.9, gamma='scale',\n",
    "#      kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False),\n",
    "#  'domain2_var2': SVR(C=1000, cache_size=200, coef0=0.0, degree=3, epsilon=1.1, gamma='scale',\n",
    "#      kernel='linear', max_iter=-1, shrinking=True, tol=0.001, verbose=False)}\n",
    "X_train = loading[loading.Id.isin(train_scores.Id)].set_index('Id').astype(np.float)\n",
    "X_for_pred = loading[~loading.Id.isin(train_scores.Id)].set_index('Id').astype(np.float)\n",
    "y_train = train_scores.astype(np.float).set_index('Id').astype(np.float)\n",
    "\n",
    "# from sklearn.svm import SVR\n",
    "# from sklearn.model_selection import KFold, GridSearchCV\n",
    "\n",
    "# features = ('age', 'domain1_var1', 'domain1_var2','domain2_var1','domain2_var2')\n",
    "\n",
    "\n",
    "# model = SVR()\n",
    "# cv = KFold(n_splits = 5, shuffle=True, random_state=42)\n",
    "# grid = {'kernel': ['linear', 'rbf'], 'epsilon': [0.01, 0.9, 1.1],'C':[5, 50, 1000]}\n",
    "# gs = GridSearchCV(model, grid, n_jobs=-1, cv=cv, verbose=50, scoring='neg_mean_absolute_error')\n",
    "\n",
    "\n",
    "\n",
    "# best_models = {}\n",
    "# for col in features:\n",
    "#     gs.fit(X_train, y_train[col])\n",
    "#     best_models[col] = gs.best_estimator_\n",
    "#     print(gs.best_score_)\n",
    "\n",
    "# predictions_train = pd.DataFrame(X_train,columns=['Id'], dtype=str)\n",
    "# predictions_test = pd.DataFrame(X_for_pred,columns=['Id'], dtype=str)\n",
    "\n",
    "# for col in features:\n",
    "#         predictions_train[col] = best_models[col].predict(X_train)\n",
    "        \n",
    "# for col in features:\n",
    "#         predictions_test[col] = best_models[col].predict(X_for_pred)\n",
    "        \n",
    "\n",
    "# predictions_test = predictions_test.drop(columns='Id').reset_index('Id')\n",
    "# predictions_train = predictions_train.drop(columns='Id').reset_index('Id')\n",
    "\n",
    "# predictions_train.to_csv('IC_train.csv')\n",
    "# predictions_test.to_csv('IC_test.csv')\n",
    "\n",
    "predictions_train = pd.read_csv('/kaggle/input/rapids-fnc-ic/fnc_ic_train_pred')\n",
    "predictions_test = pd.read_csv('/kaggle/input/rapids-fnc-ic/fnc_ic_test_pred')\n",
    "\n",
    "predictions_train.columns = ['unnamed', 'Id', 'age', 'domain1_var1', 'domain1_var2','domain2_var1','domain2_var2']\n",
    "predictions_test.columns = ['unnamed', 'Id', 'age', 'domain1_var1', 'domain1_var2','domain2_var1','domain2_var2']\n",
    "\n",
    "\n",
    "res_train = pd.read_csv('/kaggle/input/res350/res_preds_traintime3.csv')\n",
    "res_test = pd.read_csv('/kaggle/input/res350/res_preds_testtime3.csv')\n",
    "\n",
    "fnc_train = pd.read_csv('/kaggle/input/fncwide5/fnc_train5.csv')\n",
    "fnc_test = pd.read_csv('/kaggle/input/fncwide5/fnc_test5.csv')\n",
    "\n",
    "# combine columns and downsize by some stupid factor (probably not nesseccary)\n",
    "\n",
    "def X_combine(col):\n",
    "    return pd.DataFrame([res_train[col], predictions_train[col]]).transpose() * 1/200\n",
    "\n",
    "def X_combine_test(col):\n",
    "    return pd.DataFrame([res_test[col], predictions_test[col]]).transpose() * 1/200\n",
    "\n",
    "X_combine('age')\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "\n",
    "# grid search through several models\n",
    "svr = SVR(cache_size=3000.0)\n",
    "gbr = GradientBoostingRegressor(max_depth=15)\n",
    "ridge = Ridge()\n",
    "cv = KFold(n_splits = 9, shuffle=True)\n",
    "model_list = [svr, gbr, ridge]\n",
    "param_svr = {'kernel': ['linear', 'rbf'], 'epsilon': [ 0.001, 0.1,0.3, 0.6, 1.5],'C':[0.1, 1, 1000]}\n",
    "param_gbr = {'n_estimators':[5, 100, 500]}\n",
    "param_ridge = {'alpha':[0.00001, 0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]}\n",
    "param_list = [param_svr, param_gbr, param_ridge]\n",
    "\n",
    "features = ['age', 'domain1_var1', 'domain1_var2','domain2_var1','domain2_var2']\n",
    "\n",
    "# def best_train(target):\n",
    "#     X_train_best = SelectKBest(f_regression, k=100).fit_transform(X_train.iloc[:, 1:]* 1/300, train_scores[target])\n",
    "#     return X_train_best\n",
    "\n",
    "# turns out that svm almost always outperforms so this loop is totally unneccessary\n",
    "best_models = {}\n",
    "for target in features:\n",
    "    print(target)\n",
    "    i = 0\n",
    "    #for i in range(3):\n",
    "    gs = GridSearchCV(model_list[i], param_list[i], n_jobs=-1, cv=cv, verbose=5, scoring='neg_mean_absolute_error')\n",
    "    gs.fit(X_combine(target), train_scores[target])\n",
    "    if i == 0:\n",
    "        best_models[target] = gs.best_estimator_, gs.best_score_\n",
    "    _, score = best_models[target]\n",
    "    if gs.best_score_ > score:\n",
    "        best_models[target] = gs.best_estimator_, gs.best_score_\n",
    "    print(gs.best_estimator_)\n",
    "    print(gs.best_score_)\n",
    "\n",
    "ensemble_pred = np.zeros((5877, 5))\n",
    "\n",
    "# beat models example for reference\n",
    "\n",
    "# {'age': (SVR(C=10, cache_size=1000.0, coef0=0.0, degree=3, epsilon=1.5, gamma='scale',\n",
    "#       kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False),\n",
    "#   -7.1568774623455225),\n",
    "#  'domain1_var1': (SVR(C=1000, cache_size=1000.0, coef0=0.0, degree=3, epsilon=0.6, gamma='scale',\n",
    "#       kernel='linear', max_iter=-1, shrinking=True, tol=0.001, verbose=False),\n",
    "#   -7.371505402344856),\n",
    "#  'domain1_var2': (SVR(C=0.1, cache_size=1000.0, coef0=0.0, degree=3, epsilon=0.001, gamma='scale',\n",
    "#       kernel='linear', max_iter=-1, shrinking=True, tol=0.001, verbose=False),\n",
    "#   -8.297672332611237),\n",
    "#  'domain2_var1': (SVR(C=1, cache_size=1000.0, coef0=0.0, degree=3, epsilon=0.3, gamma='scale',\n",
    "#       kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False),\n",
    "#   -8.514475587821579),\n",
    "#  'domain2_var2': (SVR(C=1, cache_size=1000.0, coef0=0.0, degree=3, epsilon=0.6, gamma='scale',\n",
    "#       kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False),\n",
    "#   -9.08038587009841)}\n",
    "\n",
    "\n",
    "# make submission\n",
    "\n",
    "i = 0\n",
    "for col in features:\n",
    "    model, score = best_models[col]\n",
    "    ensemble_pred[:, i] = model.predict(X_combine_test(col))\n",
    "    i += 1\n",
    "ensemble_pred\n",
    "\n",
    "sample_sub = pd.read_csv('/kaggle/input/trends-assessment-prediction/sample_submission.csv')\n",
    "\n",
    "sample_sub['Predicted'] = ensemble_pred.flatten()\n",
    "\n",
    "\n",
    "sample_sub.head()\n",
    "\n",
    "sample_sub.to_csv('ensemble_sub_14.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
